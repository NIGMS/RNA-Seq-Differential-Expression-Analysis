{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "99cfa998-06b6-4b42-ae3a-b4e011750d31",
   "metadata": {},
   "source": [
    "# RNA-Seq Analysis using Snakemake and AWS Healthomics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4126cb07-34ee-4780-838f-872015a882b3",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f15ea992-faa6-4705-8384-eb5d81f5daff",
   "metadata": {},
   "source": [
    "This short tutorial demonstrates how to run an RNA-Seq workflow using a prokaryotic data set. Steps in the workflow include read trimming, read QC, read mapping, and counting mapped reads per gene to quantitate gene expression. This tutorial uses a popular workflow manager called [Nextflow](https://www.nextflow.io) run via [AWS Healthomics](https://aws.amazon.com/healthomics/). If you completed the other tutorials in this repo, you will see that it is similar to Tutorial 2, but instead of running Snakemake locally, we switch to Nextflow and run it using Healthomics in a serverless manner. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1bccd20",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "#### Python requirements\n",
    "+ Python >= 3.8\n",
    "#### Packages:\n",
    "+ boto3 >= 1.26.19\n",
    "+ botocore >= 1.29.19\n",
    "#### AWS requirements\n",
    "+ AWS CLI\n",
    "+ You will need the AWS CLI installed and configured in your environment. Supported AWS CLI versions are:\n",
    "    - AWS CLI v2 >= 2.9.3 (Recommended)\n",
    "    - AWS CLI v1 >= 1.27.19\n",
    "    - AWS Region\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>NOTE:</b> AWS HealthOmics only allows importing data within the same region. AWS HealthOmics is currently available in Oregon (us-west-2), N. Virginia (us-east-1), Dublin (eu-west-1), London (eu-west-2), Frankfurt (eu-central-1), and Singapore (ap-southeast-1).</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beddadd4",
   "metadata": {},
   "source": [
    "## Getting Started\n",
    "### Step 1. Import libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "345653ef",
   "metadata": {},
   "source": [
    "##### Import relevant libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac1033e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import necessary libraries and python SDK\n",
    "from datetime import datetime\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "\n",
    "import boto3\n",
    "import botocore.exceptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8221665f",
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_name = <REPLACE>\n",
    "bucket_name_out = bucket_name+\"-out\"\n",
    "account_id = boto3.client('sts').get_caller_identity().get('Account')\n",
    "region = boto3.session.Session().region_name\n",
    "workflow_name = 'rnaseq-diff-exp-workflow-test'\n",
    "# We will use this as the base name for our role and policy\n",
    "omics_iam_name = 'SageMaker_HealthOmics'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b6d0785-2d13-476c-b16a-196f74ea277d",
   "metadata": {},
   "source": [
    "### Step 2: Create a new S3 bucket to store input and output files and enable AWS Healthomics\n",
    "Note that your bucket has to be globally unique, so make sure you don't just copy the example here or it won't work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d3dc88f-fa0c-4e7e-972b-055321d3cdbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change this bucket name\n",
    "!aws s3 mb s3://$bucket_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8709238",
   "metadata": {},
   "source": [
    "### Step 3. Stage and package Workflow into .zip Folder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02c6de54",
   "metadata": {},
   "source": [
    "Clone base repos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2060017d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/nf-core/scrnaseq --branch 2.3.0 --single-branch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da6515c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/aws-samples/amazon-omics-tutorials.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5bb85e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # If you do not have the omx-ecr-helper github repository in your current repository\n",
    "# # clone the repository\n",
    "# !git clone https://github.com/CBIIT/omx-ecr-helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d06b73ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp ./omx-ecr-helper/lib/lambda/parse-image-uri/public_registry_properties.json scrnaseq/namespace.config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef5d710",
   "metadata": {},
   "source": [
    "## Generate omics.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b20b5ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate manifest and omics.config files\n",
    "!python3 amazon-omics-tutorials/utils/scripts/inspect_nf.py \\\n",
    "--output-manifest-file scrnaseq/scrnaseq_230_docker_image_manifest.json \\\n",
    "-n scrnaseq/namespace.config \\\n",
    "--output-config-file scrnaseq/conf/omics.config \\\n",
    "--region $region \\\n",
    "scrnaseq/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab688f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pull containers from manifest file generated in last step into ECR\n",
    "!aws stepfunctions start-execution\\\n",
    "    --state-machine-arn arn:aws:states:$region:$account_id:stateMachine:omx-container-puller\\\n",
    "    --input file://scrnaseq/scrnaseq_230_docker_image_manifest.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed7592e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#write omics.config statement to bottom of file\n",
    "!echo \"includeConfig 'conf/omics.config'\" >> scrnaseq/nextflow.config "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b886ee43",
   "metadata": {},
   "source": [
    "### Step 4. Create parameter-description.json file\n",
    "Run the code cell below to write the following *.json* formatted content to a *parameter-description.json* file.\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"input\": {\"description\": \"Samplesheet with sample locations.\",\n",
    "                \"optional\": false},\n",
    "    \"protocol\" : {\"description\": \"10X Protocol used: 10XV1, 10XV2, 10XV3\",\n",
    "                \"optional\": false},\n",
    "    \"aligner\": {\"description\": \"choice of aligner: alevin, star, kallisto\",\n",
    "            \"optional\": false},\n",
    "    \"whitelist\": {\"description\": \"Optional whitelist if 10X protocol is not used.\",\n",
    "            \"optional\": true},\n",
    "    \"gtf\": {\"description\": \"S3 path to GTF file\",\n",
    "            \"optional\": false},\n",
    "    \"fasta\": {\"description\": \"S3 path to FASTA file\",\n",
    "            \"optional\": false}\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "862dbadc",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('parameter-description.json',\"w\") as f:\n",
    "    f.write(json.dumps({\n",
    "        \"input\": {\"description\": \"Samplesheet with sample locations.\",\n",
    "                    \"optional\": False},\n",
    "        \"protocol\" : {\"description\": \"10X Protocol used: 10XV1, 10XV2, 10XV3\",\n",
    "                    \"optional\": False},\n",
    "        \"aligner\": {\"description\": \"choice of aligner: alevin, star, kallisto\",\n",
    "                \"optional\": False},\n",
    "        \"whitelist\": {\"description\": \"Optional whitelist if 10X protocol is not used.\",\n",
    "                \"optional\": True},\n",
    "        \"gtf\": {\"description\": \"S3 path to GTF file\",\n",
    "                \"optional\": False},\n",
    "        \"fasta\": {\"description\": \"S3 path to FASTA file\",\n",
    "                \"optional\": False}\n",
    "    }))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6555237e",
   "metadata": {},
   "source": [
    "### Step 5. Stage the Workflow\n",
    "Zip the contents of the workflow directory and copy it to an S3 bucket. If the zipped folder is >4Mb than it is required to move it to an S3 bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c132adfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "!zip -r scrnaseq-workflow.zip scrnaseq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc3a03ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 cp scrnaseq-workflow.zip s3://$bucket_name/demo_workflow/scrnaseq-workflow.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e461ec1b",
   "metadata": {},
   "source": [
    "### Step 6. Create Workflow using zipped workflow and parameters-description.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a350cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws omics create-workflow \\\n",
    "    --name $workflow_name \\\n",
    "    --definition-uri s3://$bucket_name/demo_workflow/scrnaseq-workflow.zip \\\n",
    "    --parameter-template file://parameter-description.json  \\\n",
    "    --engine NEXTFLOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5633df12",
   "metadata": {},
   "outputs": [],
   "source": [
    "#see workflow and make sure status is Active\n",
    "!aws omics list-workflows --name $workflow_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d5ae8db",
   "metadata": {},
   "source": [
    "Retrieve Workflow ID and create workflow_name variable to be passed to start_run command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b083df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = boto3.client('omics')\n",
    "workflow_id = client.list_workflows(\n",
    "    type='PRIVATE',\n",
    "    name=workflow_name,\n",
    ")['items'][0]['id']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de31bcd9",
   "metadata": {},
   "source": [
    "### Step 7. Setup Inputs\n",
    "Write *input.json* file that specifies input parameter values. Here were are retrieving inputs from public S3 buckets, however, inputs can also be passed in from your own S3 buckets or reference and genome stores that you have setup on the account. In each case just provide the appropriate uri for the given input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a83350d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dd7ab630-955d-43d1-bc43-c7b3e701ed04",
   "metadata": {},
   "source": [
    "### STEP 2: Install mambaforge and nextflow\n",
    "First install mambaforge, then use mamba to install nextflow. Skip this as needed if you have already completed this step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "682ddf88-e1d9-443f-a423-e1f85ff604a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "! curl -L -O https://github.com/conda-forge/miniforge/releases/latest/download/Mambaforge-$(uname)-$(uname -m).sh\n",
    "! bash Mambaforge-$(uname)-$(uname -m).sh -b -p $HOME/mambaforge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de5d0630-1d85-4625-bc04-036aae11ce4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add to your path, do this every time you restart your kernel\n",
    "import os\n",
    "os.environ[\"PATH\"] += os.pathsep + os.environ[\"HOME\"]+\"/mambaforge/bin\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd5c221b-45ce-47fb-a8e2-29ceee0e296a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Nextflow\n",
    "! mamba install -y -c conda-forge -c bioconda nextflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb0ce8d5-4b96-4e97-88ed-44e8e85f4fc0",
   "metadata": {},
   "source": [
    "### STEP 3: Review input files\n",
    "In order for this tutorial to run quickly, we will only analyze 50,000 reads from a sample from both sample groups instead of analyzing all the reads from all six samples. These files have been posted on a Google Storage Bucket that we made publicly accessible. All other files needed to run the pipeline are also hosted in this public bucket, and will be staged at runtime by Nextflow. To view the locations of all these files, view the `nextflow.config`. You can modify any of these paths as desired, and you could also create a new samplesheet.csv if you want to point the pipeline to different samples. The samplesheet can be stored locally or in a GS bucket."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc33d41c-3444-4e15-944f-07a8625611b7",
   "metadata": {},
   "source": [
    "### STEP 4: Modify config file to allow your project to interact with Google Batch\n",
    "Create and modify your own config file to include a 'gbatch' profile block to tell Nextflow to submit the job to Google Batch instead of running locally.\n",
    "\n",
    "The config file allows nextflow to utilize excecuters like Google Batch. In this tutorial the config files is named 'nextflow.config'. Make sure you open this file and update the <VARIABLES> that are account specific. In this case will will only modify the <PROJECT> with your Project ID. We will specify an outdir and work directory on the command line at run time. \n",
    "\n",
    "Make sure that your region is a region included in the Google Batch!\n",
    "Specify the machine type you would like to use, ensuring that there is enough memory and cpus for the workflow. In this case 16 CPUs is plenty (Otherwise Google Batch will automatically use 1 CPU).\n",
    "```\n",
    "profiles{\n",
    "  gbatch{\n",
    "      process.executor = 'google-batch'\n",
    "      google.location = 'us-central1'\n",
    "      google.region  = 'us-central1'\n",
    "      google.project = '<YOUR_PROJECT>'\n",
    "      process.machineType = 'c2-standard-16'\n",
    "     }\n",
    "}\n",
    "```\n",
    "Note: Make sure your working directory and output directory are different! Google Batch creates temporary file in the working directory within your bucket that do take up space so once your pipeline has completed succesfully feel free to delete the temporary files."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec2c0273-c7f1-4aee-bdf3-43d5773cf2fa",
   "metadata": {},
   "source": [
    "### STEP 5: Submit Nextflow Job to Google Batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc8cb07a-7c7e-430a-9781-600553b3a1e1",
   "metadata": {},
   "source": [
    "A few things to note here: \n",
    "+ --input points to a samplesheet in GS. We could also point to a local samplesheet. This just tells Nextflow where to get the fastq files. \n",
    "+ The profile comes from nextflow.config. It tells the pipeline what to use as execution environment (conda, singularity, or docker) and then you give it a compute environment (in this case gbatch, but if left blank would run locally). \n",
    "+ We specify an outdir. This can point to a local folder if run locally, but since we are using the serverless Google Batch, we need to point the output to a bucket. \n",
    "+ We specify a work dir. Like the outdir, this can be local if run locally, but needs to be in a bucket when running with Batch. \n",
    "+ If you need to rerun your pipeline, you can always add `-resume` and it will search the workdir and not rerun any processes that you have already run. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bee32318-33df-43b2-98bc-5eb091ceae59",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "! nextflow run main.nf --input gs://nigms-sandbox/me-inbre-rnaseq-pipelinev2/data/raw_fastqSub/samplesheet.csv  -profile docker,gbatch  --outdir gs://$BUCKET/outdir/ -w gs://$BUCKET/work/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df9deb0a-1030-4839-aa16-37c3b32a2c87",
   "metadata": {},
   "source": [
    "### STEP 9: Report the top 10 most highly expressed genes in the samples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d50f9bd2-dbd2-467f-a9b6-313e63ad304b",
   "metadata": {},
   "source": [
    "Top 10 most highly expressed genes in the wild-type sample. The level of expression is reported in the Transcripts Per Million (`TPM`) and number of reads (`NumReads`) fields:  \n",
    "`Name    Length  EffectiveLength TPM     NumReads`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b032ce69-f62d-4f5f-90a3-68c2979d9a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil ls gs://$BUCKET/outdir/data/quant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c98fd827-6829-400d-af8c-969ad196c3d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil cp -r gs://$BUCKET/outdir/data/quant ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fda31107-db3b-4cc2-8183-fc12bfa34e12",
   "metadata": {},
   "source": [
    "View the top 10 most highly expressed genes in the double lysogen sample.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c367b8-5764-4a49-94a5-6f59e3834821",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "for samp in quant/*/quant.sf; \n",
    "    do echo $samp; \n",
    "    sort -nrk 5,5 quant/*/quant.sf | head -10; \n",
    "    done"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50169f62-e707-4d84-b301-ded51a704130",
   "metadata": {},
   "source": [
    "### STEP 10: Report the expression of a putative acyl-ACP desaturase (BB28_RS16545) that was downregulated in the double lysogen relative to wild-type\n",
    "A acyl-transferase was reported to be downregulated in the double lysogen as shown in the table of the top 20 upregulated and downregulated genes from the paper describing the study.\n",
    "![RNA-Seq workflow](images/table-cushman.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b3794b0-a477-45fa-aa51-4414d7671441",
   "metadata": {},
   "source": [
    "Use `grep` to report the expression in the wild-type sample. The fields in the Salmon `quant.sf` file are as follows. The level of expression is reported in the Transcripts Per Million (`TPM`) and number of reads (`NumReads`) fields:  \n",
    "`Name    Length  EffectiveLength TPM     NumReads`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3cb9340-682b-4177-837d-7d803a9775a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "for samp in quant/*/quant.sf; do echo $samp; \n",
    "    echo Name    Length  EffectiveLength TPM     NumReads;\n",
    "    grep 'BB28_RS16545' quant/*/quant.sf; \n",
    "    done"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
