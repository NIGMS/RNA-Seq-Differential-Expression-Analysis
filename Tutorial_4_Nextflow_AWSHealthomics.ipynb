{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "99cfa998-06b6-4b42-ae3a-b4e011750d31",
   "metadata": {},
   "source": [
    "# RNA-Seq Analysis using Snakemake and AWS Healthomics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4126cb07-34ee-4780-838f-872015a882b3",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f15ea992-faa6-4705-8384-eb5d81f5daff",
   "metadata": {},
   "source": [
    "This short tutorial demonstrates how to run an RNA-Seq workflow using a prokaryotic data set. Steps in the workflow include read trimming, read QC, read mapping, and counting mapped reads per gene to quantitate gene expression. This tutorial uses a popular workflow manager called [Nextflow](https://www.nextflow.io) run via [AWS Healthomics](https://aws.amazon.com/healthomics/). If you completed the other tutorials in this repo, you will see that it is similar to Tutorial 2, but instead of running Snakemake locally, we switch to Nextflow and run it using Healthomics in a serverless manner. \n",
    "\n",
    "Before begining this tutorial, if you do not have ECR setup in AWS Sagemaker notebook, please click [here](https://github.com/NIGMS/AWS-HealthOmics-Module-Template/blob/scrnaseq_demo/healthomics_ecr_setup_scrnaseq.ipynb) to set that up."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1bccd20",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "#### Python requirements\n",
    "+ Python >= 3.8\n",
    "#### Packages:\n",
    "+ boto3 >= 1.26.19\n",
    "+ botocore >= 1.29.19\n",
    "#### AWS requirements\n",
    "+ AWS CLI\n",
    "+ You will need the AWS CLI installed and configured in your environment. Supported AWS CLI versions are:\n",
    "    - AWS CLI v2 >= 2.9.3 (Recommended)\n",
    "    - AWS CLI v1 >= 1.27.19\n",
    "    - AWS Region\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>NOTE:</b> AWS HealthOmics only allows importing data within the same region. AWS HealthOmics is currently available in Oregon (us-west-2), N. Virginia (us-east-1), Dublin (eu-west-1), London (eu-west-2), Frankfurt (eu-central-1), and Singapore (ap-southeast-1).</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beddadd4",
   "metadata": {},
   "source": [
    "## Getting Started\n",
    "### Step 1. Import libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "345653ef",
   "metadata": {},
   "source": [
    "##### Import relevant libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac1033e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries and python SDK\n",
    "from datetime import datetime\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "\n",
    "import boto3\n",
    "import botocore.exceptions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d92eb872",
   "metadata": {},
   "source": [
    "For AWS bucket naming conventions, please click [here](https://docs.aws.amazon.com/AmazonS3/latest/userguide/bucketnamingrules.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8221665f",
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_name = <REPLACE with bucket name>\n",
    "bucket_name_out = bucket_name+\"-out\"\n",
    "account_id = boto3.client('sts').get_caller_identity().get('Account')\n",
    "region = boto3.session.Session().region_name\n",
    "workflow_name = <REPLACE with workflow name>\n",
    "# We will use this as the base name for our role and policy\n",
    "omics_iam_name = <REPLACE with omics IAM name>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b6d0785-2d13-476c-b16a-196f74ea277d",
   "metadata": {},
   "source": [
    "### Step 2: Create a new S3 bucket to store input and output files and enable AWS Healthomics\n",
    "Note that your bucket has to be globally unique, so make sure you don't just copy the example here or it won't work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d3dc88f-fa0c-4e7e-972b-055321d3cdbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will use the bucket name variable from above\n",
    "!aws s3 mb s3://$bucket_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b305ed2",
   "metadata": {},
   "source": [
    "### Step 3: Review input files\n",
    "In order for this tutorial to run quickly, we will only analyze 50,000 reads from a sample from both sample groups instead of analyzing all the reads from all six samples. These files have been posted on a AWS S3 Storage Bucket that we made publicly accessible. All other files needed to run the pipeline are also hosted in this public bucket, and will be staged at runtime by Nextflow. To view the locations of all these files, view the `nextflow.config`. You can modify any of these paths as desired, and you could also create a new samplesheet.csv if you want to point the pipeline to different samples. The samplesheet can be stored locally or in a S3 bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec0cf36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # If downloading locally, downloaded necessary data otherwise skip this step\n",
    "# !aws s3 cp s3://nigms-sandbox/me-inbre-rnaseq-pipelinev2/data/ data/ --recursive"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8709238",
   "metadata": {},
   "source": [
    "### Step 4. Stage and package Workflow into .zip Folder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02c6de54",
   "metadata": {},
   "source": [
    "Clone base repos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2060017d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/nf-core/rnaseq --branch 3.11.0 --single-branch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da6515c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/aws-samples/amazon-omics-tutorials.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5bb85e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # If you do not have the omx-ecr-helper github repository in your current repository\n",
    "# # clone the repository\n",
    "# !git clone https://github.com/CBIIT/omx-ecr-helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d06b73ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy namespace.config file to generate omics.config file\n",
    "!cp ./omx-ecr-helper/lib/lambda/parse-image-uri/public_registry_properties.json rnaseq/namespace.config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef5d710",
   "metadata": {},
   "source": [
    "## Generate omics.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b20b5ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate manifest and omics.config files\n",
    "!python3 amazon-omics-tutorials/utils/scripts/inspect_nf.py \\\n",
    "--output-manifest-file rnaseq/rnaseq_docker_image_manifest.json \\\n",
    "-n rnaseq/namespace.config \\\n",
    "--output-config-file rnaseq/conf/omics.config \\\n",
    "--region $region \\\n",
    "rnaseq/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab688f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pull containers from manifest file generated in last step into ECR\n",
    "!aws stepfunctions start-execution\\\n",
    "    --state-machine-arn arn:aws:states:$region:$account_id:stateMachine:omx-container-puller\\\n",
    "    --input file://rnaseq/rnaseq_docker_image_manifest.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed7592e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write omics.config statement to bottom of file. This should only be run once otherwise multiple statements will be added to nextflow.config file\n",
    "!echo \"includeConfig 'conf/omics.config'\" >> rnaseq/nextflow.config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b886ee43",
   "metadata": {},
   "source": [
    "### Step 5. Create parameter-description.json file\n",
    "Run the code cell below to write the following *.json* formatted content to a *parameter-description.json* file.\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"input\": {\"description\": \"(string)Path to comma-separated file containing information about the samples in the experiment. Samplesheet with sample locations.\",\n",
    "                \"optional\": false},\n",
    "    \"ecr_registry\": {\n",
    "        \"description\": \"(string)Name of ECR private registry containing docker containers.\",\n",
    "        \"optional\": false},\n",
    "    \"aligner\": {\"description\": \"(string[star_salmon|star_rsem|hisat2]:star_salmon)Specifies the alignment algorithm to use.choice of aligner: alevin, star, kallisto\",\n",
    "            \"optional\": true},\n",
    "    \"gtf\": {\"description\": \"(string)Path to GTF annotation file. S3 path to GTF file\",\n",
    "            \"optional\": true},\n",
    "    \"fasta\": {\"description\": \"(string)Path to FASTA genome file. S3 path to FASTA file\",\n",
    "            \"optional\": false}\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "862dbadc",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('parameter-description.json',\"w\") as f:\n",
    "    f.write(json.dumps({\n",
    "    \"input\": {\"description\": \"(string)Path to comma-separated file containing information about the samples in the experiment. Samplesheet with sample locations.\",\n",
    "                \"optional\": False},\n",
    "    \"ecr_registry\": {\n",
    "        \"description\": \"(string)Name of ECR private registry containing docker containers.\",\n",
    "        \"optional\": False},\n",
    "    \"aligner\": {\"description\": \"(string[star_salmon|star_rsem|hisat2]:star_salmon)Specifies the alignment algorithm to use.choice of aligner: alevin, star, kallisto\",\n",
    "            \"optional\": True},\n",
    "    \"gtf\": {\"description\": \"(string)Path to GTF annotation file. S3 path to GTF file\",\n",
    "            \"optional\": True},\n",
    "    \"fasta\": {\"description\": \"(string)Path to FASTA genome file. S3 path to FASTA file\",\n",
    "            \"optional\": False}\n",
    "    }))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6555237e",
   "metadata": {},
   "source": [
    "### Step 6. Stage the Workflow\n",
    "Zip the contents of the workflow directory and copy it to an S3 bucket. If the zipped folder is >4Mb than it is required to move it to an S3 bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c132adfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "!zip -r rnaseq-workflow.zip rnaseq "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc3a03ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload to S3 bucket\n",
    "!aws s3 cp rnaseq-workflow.zip s3://$bucket_name/rnaseq-workflow.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e461ec1b",
   "metadata": {},
   "source": [
    "### Step 7. Create Workflow using zipped workflow and parameters-description.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a350cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws omics create-workflow \\\n",
    "    --name $workflow_name \\\n",
    "    --definition-uri s3://$bucket_name/rnaseq-workflow.zip \\\n",
    "    --parameter-template file://parameter-description.json  \\\n",
    "    --engine NEXTFLOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5633df12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# See workflow and make sure status is Active\n",
    "!aws omics list-workflows --name $workflow_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d5ae8db",
   "metadata": {},
   "source": [
    "Retrieve Workflow ID and create workflow_name variable to be passed to start_run command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b083df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = boto3.client('omics')\n",
    "workflow_id = client.list_workflows(\n",
    "    type='PRIVATE',\n",
    "    name=workflow_name,\n",
    ")['items'][0]['id']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de31bcd9",
   "metadata": {},
   "source": [
    "### Step 8. Setup Inputs\n",
    "Write *input.json* file that specifies input parameter values. Here were are retrieving inputs from your own S3 buckets, however, inputs can also be passed in from public S3 buckets or reference and genome stores that you have setup on the account. In each case just provide the appropriate uri for the given input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a83350d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('input.json',\"w\") as f:\n",
    "    f.write(json.dumps({\n",
    "        \"input\": \"s3://nigms-sandbox/me-inbre-rnaseq-pipelinev2/data/raw_fastqSub/samplesheet.csv\",\n",
    "        \"ecr_registry\": account_id + \".dkr.ecr.\"+region+\".amazonaws.com/\"+workflow_name\n",
    "}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a71ed21c",
   "metadata": {},
   "source": [
    "### Step 9. Setup new role\n",
    "For the purposes of this demo, we will use the following policy and trust policy that restricts usage to only the required S3 buckets. You will need to customize permissions as required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0899d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define demo policies\n",
    "omics_demo_policy = {\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Statement\": [\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": [\n",
    "                \"s3:GetObject\"\n",
    "            ],\n",
    "            \"Resource\": [\n",
    "                \"arn:aws:s3:::\"+bucket_name+\"/*\",\n",
    "                \"arn:aws:s3:::\"+bucket_name_out+\"/*\",\n",
    "                \"arn:aws:s3:::nigms-sandbox/me-inbre-rnaseq-pipelinev2/data/*\" \n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": [\n",
    "                \"s3:ListBucket\"\n",
    "            ],\n",
    "            \"Resource\": [\n",
    "                \"arn:aws:s3:::\"+bucket_name,\n",
    "                \"arn:aws:s3:::\"+bucket_name_out+\"/*\",\n",
    "                \"arn:aws:s3:::nigms-sandbox/me-inbre-rnaseq-pipelinev2/data\"\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": [\n",
    "                \"s3:PutObject\"\n",
    "            ],\n",
    "            \"Resource\": [\n",
    "                \"arn:aws:s3:::\"+bucket_name+\"/*\",\n",
    "                \"arn:aws:s3:::\"+bucket_name_out+\"/*\",\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": [\n",
    "                \"logs:DescribeLogStreams\",\n",
    "                \"logs:CreateLogStream\",\n",
    "                \"logs:PutLogEvents\"\n",
    "            ],\n",
    "            \"Resource\": [\n",
    "                \"arn:aws:logs:\"+region+\":\"+account_id+\":log-group:/aws/omics/WorkflowLog:log-stream:*\"\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": [\n",
    "                \"logs:CreateLogGroup\"\n",
    "            ],\n",
    "            \"Resource\": [\n",
    "                \"arn:aws:logs:\"+region+\":\"+account_id+\":log-group:/aws/omics/WorkflowLog:*\"\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": [\n",
    "                \"ecr:BatchGetImage\",\n",
    "                \"ecr:GetDownloadUrlForLayer\",\n",
    "                \"ecr:BatchCheckLayerAvailability\"\n",
    "            ],\n",
    "            \"Resource\": [\n",
    "                \"arn:aws:ecr:\"+region+\":\"+account_id+\":repository/*\"\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "rnaseq_workflow_demo_trust_policy = {\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Statement\": [\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Principal\": {\n",
    "                \"Service\": \"omics.amazonaws.com\"\n",
    "            },\n",
    "            \"Action\": \"sts:AssumeRole\",\n",
    "            \"Condition\": {\n",
    "                \"StringEquals\": {\n",
    "                    \"aws:SourceAccount\": account_id\n",
    "                },\n",
    "                \"ArnLike\": {\n",
    "                    \"aws:SourceArn\": \"arn:aws:omics:\"+region+\":\"+account_id+\":run/*\"\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d903949c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the iam client\n",
    "iam = boto3.resource('iam')\n",
    "\n",
    "# Check if the role already exists; if not, create it\n",
    "try:\n",
    "    role = iam.Role(omics_iam_name)\n",
    "    role.load()\n",
    "    \n",
    "except botocore.exceptions.ClientError as ex:\n",
    "    if ex.response[\"Error\"][\"Code\"] == \"NoSuchEntity\":\n",
    "        # Create the role with the corresponding trust policy\n",
    "        role = iam.create_role(\n",
    "            RoleName=omics_iam_name, \n",
    "            AssumeRolePolicyDocument=json.dumps(rnaseq_workflow_demo_trust_policy))\n",
    "        \n",
    "        # Create policy\n",
    "        policy = iam.create_policy(\n",
    "            PolicyName='{}-policy'.format(omics_iam_name), \n",
    "            Description=\"Policy for AWS HealthOmics demo\",\n",
    "            PolicyDocument=json.dumps(omics_demo_policy))\n",
    "        \n",
    "        # Attach the policy to the role\n",
    "        policy.attach_role(RoleName=omics_iam_name)\n",
    "    else:\n",
    "        print('Something went wrong, please retry and check your account settings and permissions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "126eca56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the role arn, which grants AWS HealthOmics the proper permissions to access the resources it needs in your AWS account.\n",
    "def get_role_arn(role_name):\n",
    "    try:\n",
    "        iam = boto3.resource('iam')\n",
    "        role = iam.Role(role_name)\n",
    "        role.load()  # calls GetRole to load attributes\n",
    "    except botocore.exceptions.ClientError:\n",
    "        print(\"Couldn't get role named %s.\"%role_name)\n",
    "        raise\n",
    "    else:\n",
    "        print(role.arn)\n",
    "        return role.arn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "655a6dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print role name and role arn to be used in store creation and upload\n",
    "role_arn = get_role_arn(omics_iam_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "286214dc",
   "metadata": {},
   "source": [
    "### Step 10. Start the run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8973a33b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws omics start-run \\\n",
    "  --name <REPLACE with run name> \\\n",
    "  --role-arn $role_arn \\\n",
    "  --workflow-id $workflow_id \\\n",
    "  --parameters file://input.json \\\n",
    "  --output-uri s3://$bucket_name_out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd7ab630-955d-43d1-bc43-c7b3e701ed04",
   "metadata": {},
   "source": [
    "### STEP 2: Install mambaforge and nextflow\n",
    "First install mambaforge, then use mamba to install nextflow. Skip this as needed if you have already completed this step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "682ddf88-e1d9-443f-a423-e1f85ff604a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "! curl -L -O https://github.com/conda-forge/miniforge/releases/latest/download/Mambaforge-$(uname)-$(uname -m).sh\n",
    "! bash Mambaforge-$(uname)-$(uname -m).sh -b -p $HOME/mambaforge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de5d0630-1d85-4625-bc04-036aae11ce4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add to your path, do this every time you restart your kernel\n",
    "import os\n",
    "os.environ[\"PATH\"] += os.pathsep + os.environ[\"HOME\"]+\"/mambaforge/bin\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd5c221b-45ce-47fb-a8e2-29ceee0e296a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Nextflow\n",
    "! mamba install -y -c conda-forge -c bioconda nextflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb0ce8d5-4b96-4e97-88ed-44e8e85f4fc0",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dc33d41c-3444-4e15-944f-07a8625611b7",
   "metadata": {},
   "source": [
    "### STEP 4: Modify config file to allow your project to interact with Google Batch\n",
    "Create and modify your own config file to include a 'gbatch' profile block to tell Nextflow to submit the job to Google Batch instead of running locally.\n",
    "\n",
    "The config file allows nextflow to utilize excecuters like Google Batch. In this tutorial the config files is named 'nextflow.config'. Make sure you open this file and update the <VARIABLES> that are account specific. In this case will will only modify the <PROJECT> with your Project ID. We will specify an outdir and work directory on the command line at run time. \n",
    "\n",
    "Make sure that your region is a region included in the Google Batch!\n",
    "Specify the machine type you would like to use, ensuring that there is enough memory and cpus for the workflow. In this case 16 CPUs is plenty (Otherwise Google Batch will automatically use 1 CPU).\n",
    "```\n",
    "profiles{\n",
    "  gbatch{\n",
    "      process.executor = 'google-batch'\n",
    "      google.location = 'us-central1'\n",
    "      google.region  = 'us-central1'\n",
    "      google.project = '<YOUR_PROJECT>'\n",
    "      process.machineType = 'c2-standard-16'\n",
    "     }\n",
    "}\n",
    "```\n",
    "Note: Make sure your working directory and output directory are different! Google Batch creates temporary file in the working directory within your bucket that do take up space so once your pipeline has completed succesfully feel free to delete the temporary files."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec2c0273-c7f1-4aee-bdf3-43d5773cf2fa",
   "metadata": {},
   "source": [
    "### STEP 5: Submit Nextflow Job to Google Batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc8cb07a-7c7e-430a-9781-600553b3a1e1",
   "metadata": {},
   "source": [
    "A few things to note here: \n",
    "+ --input points to a samplesheet in GS. We could also point to a local samplesheet. This just tells Nextflow where to get the fastq files. \n",
    "+ The profile comes from nextflow.config. It tells the pipeline what to use as execution environment (conda, singularity, or docker) and then you give it a compute environment (in this case gbatch, but if left blank would run locally). \n",
    "+ We specify an outdir. This can point to a local folder if run locally, but since we are using the serverless Google Batch, we need to point the output to a bucket. \n",
    "+ We specify a work dir. Like the outdir, this can be local if run locally, but needs to be in a bucket when running with Batch. \n",
    "+ If you need to rerun your pipeline, you can always add `-resume` and it will search the workdir and not rerun any processes that you have already run. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bee32318-33df-43b2-98bc-5eb091ceae59",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "! nextflow run main.nf --input gs://nigms-sandbox/me-inbre-rnaseq-pipelinev2/data/raw_fastqSub/samplesheet.csv  -profile docker,gbatch  --outdir gs://$BUCKET/outdir/ -w gs://$BUCKET/work/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df9deb0a-1030-4839-aa16-37c3b32a2c87",
   "metadata": {},
   "source": [
    "### STEP 9: Report the top 10 most highly expressed genes in the samples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d50f9bd2-dbd2-467f-a9b6-313e63ad304b",
   "metadata": {},
   "source": [
    "Top 10 most highly expressed genes in the wild-type sample. The level of expression is reported in the Transcripts Per Million (`TPM`) and number of reads (`NumReads`) fields:  \n",
    "`Name    Length  EffectiveLength TPM     NumReads`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b032ce69-f62d-4f5f-90a3-68c2979d9a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil ls gs://$BUCKET/outdir/data/quant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c98fd827-6829-400d-af8c-969ad196c3d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil cp -r gs://$BUCKET/outdir/data/quant ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fda31107-db3b-4cc2-8183-fc12bfa34e12",
   "metadata": {},
   "source": [
    "View the top 10 most highly expressed genes in the double lysogen sample.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c367b8-5764-4a49-94a5-6f59e3834821",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "for samp in quant/*/quant.sf; \n",
    "    do echo $samp; \n",
    "    sort -nrk 5,5 quant/*/quant.sf | head -10; \n",
    "    done"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50169f62-e707-4d84-b301-ded51a704130",
   "metadata": {},
   "source": [
    "### STEP 10: Report the expression of a putative acyl-ACP desaturase (BB28_RS16545) that was downregulated in the double lysogen relative to wild-type\n",
    "A acyl-transferase was reported to be downregulated in the double lysogen as shown in the table of the top 20 upregulated and downregulated genes from the paper describing the study.\n",
    "![RNA-Seq workflow](images/table-cushman.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b3794b0-a477-45fa-aa51-4414d7671441",
   "metadata": {},
   "source": [
    "Use `grep` to report the expression in the wild-type sample. The fields in the Salmon `quant.sf` file are as follows. The level of expression is reported in the Transcripts Per Million (`TPM`) and number of reads (`NumReads`) fields:  \n",
    "`Name    Length  EffectiveLength TPM     NumReads`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3cb9340-682b-4177-837d-7d803a9775a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "for samp in quant/*/quant.sf; do echo $samp; \n",
    "    echo Name    Length  EffectiveLength TPM     NumReads;\n",
    "    grep 'BB28_RS16545' quant/*/quant.sf; \n",
    "    done"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
