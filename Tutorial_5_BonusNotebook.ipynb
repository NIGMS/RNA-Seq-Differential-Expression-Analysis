{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Extended RNA-Seq Analysis Training Demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, we will repeat the Tutorial 1B extended submodule with a different dataset. Using the provided dataset is a great way to get introduced to the module and learn its basic functionality, but to use it with your own data or to understand the module at a deeper level, it is helpful to practice adapting it to a new dataset. We will guide you through that here, providing basic guidance on the workflow and letting you write the commands yourself. If you get stuck, you can always view our hints and suggestions by clicking on the dropdown arrow under the command cells.\n",
    "\n",
    "The initial dataset used a prokaryotic sample for simplicity and compute efficiency. This time, we will increase the complexity a bit by using a eukaryotic sample. We have selected data from a time series experiment on Plasmodium falciparum, the parasite responsible for malaria. The experiment did RNA-seq analysis on P. falciparum cells at 25 time points after erythrocyte invasion. The data is taken from Kucharski M, Tripathi J, Nayak S, Zhu L et al. A comprehensive RNA handling and transcriptomics guide for high-throughput processing of Plasmodium blood-stage samples. Malar J 2020 Oct 9;19(1):363. The sequence data is available from SRA with the accession number [SRP261441](https://www.ncbi.nlm.nih.gov/sra/?term=SRP261441). To keep things simple, we are not using all time points, and have selected time points 1, 13, and 25 as a proxy for early, mid, and late infection. Feel free to add or remove samples from your analysis to see how the results differ. The workflow structure remains the same as the original submodule 1B, with the diagram shown below.\n",
    "\n",
    "![RNA-Seq workflow](images/rnaseq-workflow.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 1: Install Mambaforge and then install snakemake using bioconda."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First install Mambaforge.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<YOUR COMMAND HERE>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<details>\n",
    "  <summary>Click for help</summary>\n",
    "    \n",
    "  Make sure you include the `!` in front of your command! \n",
    "    \n",
    "  ```  \n",
    "curl -L -O https://github.com/conda-forge/miniforge/releases/latest/download/Mambaforge-$(uname)-$(uname -m).sh\n",
    "bash Mambaforge-$(uname)-$(uname -m).sh -b -u -p $HOME/mambaforge\n",
    "date +\"%T\"\n",
    "  ```\n",
    "\n",
    "</details>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, add it to the path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2048400170.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"/var/tmp/ipykernel_7220/2048400170.py\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    <YOUR COMMAND HERE>\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "<YOUR COMMAND HERE>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>Click for help</summary>\n",
    "    \n",
    "  Make sure you include the `!` in front of your command! \n",
    "    \n",
    "  ```  \n",
    "#add to your path\n",
    "import os\n",
    "os.environ[\"PATH\"] += os.pathsep + os.environ[\"HOME\"]+\"/mambaforge/bin\"  \n",
    "  ```\n",
    "\n",
    "</details>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, using mambaforge and bioconda, install the tools that will be used in this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<YOUR COMMAND HERE>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<details>\n",
    "  <summary>Click for help</summary>\n",
    "    \n",
    "  Make sure you include the `!` in front of your command! \n",
    "    \n",
    "  ```  \n",
    "  mamba install -y -c conda-forge -c bioconda trimmomatic fastqc multiqc salmon entrez-direct gffread parallel-fastq-dump sra-tools pigz\n",
    "  ```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 2: Setup Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The directory structure will be the same as in Tutorial 1B. Create that below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<YOUR COMMAND HERE>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>Click for help</summary>\n",
    "    \n",
    "  Make sure you include the `!` in front of your command! \n",
    "    \n",
    "  ```  \n",
    " cd $HOMEDIR\n",
    " mkdir -p data\n",
    " mkdir -p data/raw_fastq\n",
    " mkdir -p data/trimmed\n",
    " mkdir -p data/fastqc\n",
    " mkdir -p data/aligned\n",
    " mkdir -p data/reference\n",
    "  ```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set number of cores depending on your VM size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<YOUR COMMAND HERE>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>Click for help</summary>\n",
    "    \n",
    "  Make sure you include the `!` in front of your command! \n",
    "    \n",
    "  ```  \n",
    "\n",
    "numthreads=!nproc\n",
    "numthreadsint = int(numthreads[0])\n",
    "%env CORES = $numthreadsint\n",
    "#!echo ${CORES}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### STEP 3: Downloading relevant FASTQ files using SRA Tools\n",
    "\n",
    "Next we will need to download the relevant fastq files.\n",
    "\n",
    "Because these files can be large, the process of downloading and extracting fastq files can be quite lengthy.\n",
    "\n",
    "The sequence data for this tutorial comes from work by Cushman et al., <em><a href='https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8191103/'>Increased whiB7 expression and antibiotic resistance in Mycobacterium chelonae carrying two prophages</a><em>.\n",
    "\n",
    "We will be downloading the sample runs from this project using SRA tools, downloading from the NCBI's SRA (Sequence Run Archives).\n",
    "\n",
    "However, first we need to find the associated accession numbers in order to download.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 3.1: Finding run accession numbers.\n",
    "\n",
    "The SRA stores sequence data in terms of runs, (SRR stands for Sequence Read Run). To download runs, we will need the accession ID for each run we wish to download. \n",
    "\n",
    "The Cushman et al., project contains 12 runs. To make it easier, these are the run IDs associated with this project:\n",
    "\n",
    "+ SRR13349122\n",
    "+ SRR13349123\n",
    "+ SRR13349124\n",
    "+ SRR13349125\n",
    "+ SRR13349126\n",
    "+ SRR13349127\n",
    "+ SRR13349128\n",
    "+ SRR13349129\n",
    "+ SRR13349130\n",
    "+ SRR13349131\n",
    "+ SRR13349132\n",
    "+ SRR13349133\n",
    "\n",
    "\n",
    "In this case, all these runs belong to the SRP (Sequence Run Project): SRP300216.\n",
    "\n",
    "Sequence run experiments can be searched for using the SRA database on the NCBI website; and article-specific sample run information can be found in the supplementary section of that article.\n",
    "\n",
    "For instance, here, the the authors posted a link to the sequence data GSE (Gene Series number), <a href='https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE164210'>GSE164210</a>. This leads to the appropriate 'Gene Expression Omnibus' page where, among other useful files and information, the relevant SRA database link can be found. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 3.1.2 (Optional): Generate the accession list file with BigQuery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the biquery api\n",
    "from google.cloud import bigquery\n",
    "import pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now make sure you have enabled the BigQuery API. You just need to search for BigQuery, go to the BQ page and click `Enable`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client creating using default project: cit-oconnellka-9999\n"
     ]
    }
   ],
   "source": [
    "# Designate the client for the API\n",
    "client = bigquery.Client(location=\"US\")\n",
    "print(\"Client creating using default project: {}\".format(client.project))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will query BigQuery using the species name and a range of accession numbers associated with this particular study. Feel free to play around with the query to generate different variations of accession numbers!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "#standardSQL\n",
    "SELECT *\n",
    "FROM `nih-sra-datastore.sra.metadata`\n",
    "WHERE organism = 'Plasmodium falciparum 3D7'\n",
    "and acc IN ('SRR11784387','SRR11784398','SRR11784410')\n",
    "\"\"\"\n",
    "query_job = client.query(\n",
    "    query,\n",
    "    # Location must match that of the dataset(s) referenced in the query.\n",
    "    location=\"US\",\n",
    ")  # API request - starts the query\n",
    "\n",
    "df = query_job.to_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>acc</th>\n",
       "      <th>assay_type</th>\n",
       "      <th>center_name</th>\n",
       "      <th>consent</th>\n",
       "      <th>experiment</th>\n",
       "      <th>sample_name</th>\n",
       "      <th>instrument</th>\n",
       "      <th>librarylayout</th>\n",
       "      <th>libraryselection</th>\n",
       "      <th>librarysource</th>\n",
       "      <th>...</th>\n",
       "      <th>geo_loc_name_sam</th>\n",
       "      <th>ena_first_public_run</th>\n",
       "      <th>ena_last_update_run</th>\n",
       "      <th>sample_name_sam</th>\n",
       "      <th>datastore_filetype</th>\n",
       "      <th>datastore_provider</th>\n",
       "      <th>datastore_region</th>\n",
       "      <th>attributes</th>\n",
       "      <th>run_file_version</th>\n",
       "      <th>jattr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SRR11784387</td>\n",
       "      <td>RNA-Seq</td>\n",
       "      <td>GEO</td>\n",
       "      <td>public</td>\n",
       "      <td>SRX8336810</td>\n",
       "      <td>GSM4551251</td>\n",
       "      <td>Illumina HiSeq 4000</td>\n",
       "      <td>PAIRED</td>\n",
       "      <td>cDNA</td>\n",
       "      <td>TRANSCRIPTOMIC</td>\n",
       "      <td>...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[fastq, sra, run.zq]</td>\n",
       "      <td>[ncbi, s3, gs]</td>\n",
       "      <td>[s3.us-east-1, gs.US, ncbi.public]</td>\n",
       "      <td>[{'k': 'geo_accession_exp', 'v': 'GSM4551251'}...</td>\n",
       "      <td>1</td>\n",
       "      <td>{\"geo_accession_exp\": [\"GSM4551251\"], \"bases\":...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SRR11784410</td>\n",
       "      <td>RNA-Seq</td>\n",
       "      <td>GEO</td>\n",
       "      <td>public</td>\n",
       "      <td>SRX8336833</td>\n",
       "      <td>GSM4551274</td>\n",
       "      <td>Illumina HiSeq 4000</td>\n",
       "      <td>PAIRED</td>\n",
       "      <td>cDNA</td>\n",
       "      <td>TRANSCRIPTOMIC</td>\n",
       "      <td>...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[sra, run.zq, fastq]</td>\n",
       "      <td>[gs, s3, ncbi]</td>\n",
       "      <td>[gs.US, s3.us-east-1, ncbi.public]</td>\n",
       "      <td>[{'k': 'geo_accession_exp', 'v': 'GSM4551274'}...</td>\n",
       "      <td>1</td>\n",
       "      <td>{\"geo_accession_exp\": [\"GSM4551274\"], \"bases\":...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SRR11784398</td>\n",
       "      <td>RNA-Seq</td>\n",
       "      <td>GEO</td>\n",
       "      <td>public</td>\n",
       "      <td>SRX8336821</td>\n",
       "      <td>GSM4551262</td>\n",
       "      <td>Illumina HiSeq 4000</td>\n",
       "      <td>PAIRED</td>\n",
       "      <td>cDNA</td>\n",
       "      <td>TRANSCRIPTOMIC</td>\n",
       "      <td>...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[run.zq, sra, fastq]</td>\n",
       "      <td>[s3, ncbi, gs]</td>\n",
       "      <td>[ncbi.public, s3.us-east-1, gs.US]</td>\n",
       "      <td>[{'k': 'geo_accession_exp', 'v': 'GSM4551262'}...</td>\n",
       "      <td>1</td>\n",
       "      <td>{\"geo_accession_exp\": [\"GSM4551262\"], \"bases\":...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows √ó 37 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           acc assay_type center_name consent  experiment sample_name  \\\n",
       "0  SRR11784387    RNA-Seq         GEO  public  SRX8336810  GSM4551251   \n",
       "1  SRR11784410    RNA-Seq         GEO  public  SRX8336833  GSM4551274   \n",
       "2  SRR11784398    RNA-Seq         GEO  public  SRX8336821  GSM4551262   \n",
       "\n",
       "            instrument librarylayout libraryselection   librarysource  ...  \\\n",
       "0  Illumina HiSeq 4000        PAIRED             cDNA  TRANSCRIPTOMIC  ...   \n",
       "1  Illumina HiSeq 4000        PAIRED             cDNA  TRANSCRIPTOMIC  ...   \n",
       "2  Illumina HiSeq 4000        PAIRED             cDNA  TRANSCRIPTOMIC  ...   \n",
       "\n",
       "  geo_loc_name_sam ena_first_public_run ena_last_update_run sample_name_sam  \\\n",
       "0               []                   []                  []              []   \n",
       "1               []                   []                  []              []   \n",
       "2               []                   []                  []              []   \n",
       "\n",
       "     datastore_filetype datastore_provider  \\\n",
       "0  [fastq, sra, run.zq]     [ncbi, s3, gs]   \n",
       "1  [sra, run.zq, fastq]     [gs, s3, ncbi]   \n",
       "2  [run.zq, sra, fastq]     [s3, ncbi, gs]   \n",
       "\n",
       "                     datastore_region  \\\n",
       "0  [s3.us-east-1, gs.US, ncbi.public]   \n",
       "1  [gs.US, s3.us-east-1, ncbi.public]   \n",
       "2  [ncbi.public, s3.us-east-1, gs.US]   \n",
       "\n",
       "                                          attributes run_file_version  \\\n",
       "0  [{'k': 'geo_accession_exp', 'v': 'GSM4551251'}...                1   \n",
       "1  [{'k': 'geo_accession_exp', 'v': 'GSM4551274'}...                1   \n",
       "2  [{'k': 'geo_accession_exp', 'v': 'GSM4551262'}...                1   \n",
       "\n",
       "                                               jattr  \n",
       "0  {\"geo_accession_exp\": [\"GSM4551251\"], \"bases\":...  \n",
       "1  {\"geo_accession_exp\": [\"GSM4551274\"], \"bases\":...  \n",
       "2  {\"geo_accession_exp\": [\"GSM4551262\"], \"bases\":...  \n",
       "\n",
       "[3 rows x 37 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('accs.txt', 'w') as f:\n",
    "    accs = df['acc'].to_string(header=False, index=False)\n",
    "    f.write(accs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SRR11784387\n",
      "SRR11784410\n",
      "SRR11784398"
     ]
    }
   ],
   "source": [
    "cat accs.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### STEP 3.3 Downloading multiple files using the SRA-toolkit.\n",
    "\n",
    "One may, as in our case, wish to download multiple runs at once.\n",
    "\n",
    "To aid in this, SRA-tools supports batch downloading.\n",
    "\n",
    "We can download multiple SRA files using a single line of code by creating a list of the SRA IDs we wish to download, and inputting that into the prefetch command."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then feed that list into the sra-toolkit prefetch command. Note, it may take some time to download all the fastq files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2023-12-13T19:54:46 prefetch.3.0.9: Current preference is set to retrieve SRA Normalized Format files with full base quality scores.\n",
      "2023-12-13T19:54:46 prefetch.3.0.9: 1) Downloading 'SRR11784410'...\n",
      "2023-12-13T19:54:46 prefetch.3.0.9: SRA Normalized Format file is being retrieved, if this is different from your preference, it may be due to current file availability.\n",
      "2023-12-13T19:54:46 prefetch.3.0.9:  Downloading via HTTPS...\n",
      "2023-12-13T19:56:45 prefetch.3.0.9:  HTTPS download succeed\n",
      "2023-12-13T19:56:53 prefetch.3.0.9:  'SRR11784410' is valid\n",
      "2023-12-13T19:56:53 prefetch.3.0.9: 1) 'SRR11784410' was downloaded successfully\n",
      "2023-12-13T19:56:53 prefetch.3.0.9: 'SRR11784410' has 0 unresolved dependencies\n",
      "\n",
      "2023-12-13T19:56:53 prefetch.3.0.9: Current preference is set to retrieve SRA Normalized Format files with full base quality scores.\n",
      "2023-12-13T19:56:53 prefetch.3.0.9: 2) Downloading 'SRR11784398'...\n",
      "2023-12-13T19:56:53 prefetch.3.0.9: SRA Normalized Format file is being retrieved, if this is different from your preference, it may be due to current file availability.\n",
      "2023-12-13T19:56:53 prefetch.3.0.9:  Downloading via HTTPS...\n",
      "2023-12-13T19:58:16 prefetch.3.0.9:  HTTPS download succeed\n",
      "2023-12-13T19:58:21 prefetch.3.0.9:  'SRR11784398' is valid\n",
      "2023-12-13T19:58:21 prefetch.3.0.9: 2) 'SRR11784398' was downloaded successfully\n",
      "2023-12-13T19:58:21 prefetch.3.0.9: 'SRR11784398' has 0 unresolved dependencies\n",
      "\n",
      "2023-12-13T19:58:22 prefetch.3.0.9: Current preference is set to retrieve SRA Normalized Format files with full base quality scores.\n",
      "2023-12-13T19:58:22 prefetch.3.0.9: 3) Downloading 'SRR11784387'...\n",
      "2023-12-13T19:58:22 prefetch.3.0.9: SRA Normalized Format file is being retrieved, if this is different from your preference, it may be due to current file availability.\n",
      "2023-12-13T19:58:22 prefetch.3.0.9:  Downloading via HTTPS...\n",
      "2023-12-13T19:59:21 prefetch.3.0.9:  HTTPS download succeed\n",
      "2023-12-13T19:59:24 prefetch.3.0.9:  'SRR11784387' is valid\n",
      "2023-12-13T19:59:24 prefetch.3.0.9: 3) 'SRR11784387' was downloaded successfully\n",
      "2023-12-13T19:59:24 prefetch.3.0.9: 'SRR11784387' has 0 unresolved dependencies\n"
     ]
    }
   ],
   "source": [
    "!prefetch -O data/raw_fastq/ --option-file accs.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 3.3 Converting Multiple SRA files to Fastq\n",
    "\n",
    "We used fasterq-dump before to convert SRA files to fastq. However, fasterq-dump does not have native batch compatibility. As before, we will use a loop to convert each file in our list. In this case, we are going to convert to fastq.gz for downstream processing. This step should take about 30 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spots read      : 26,704,146\n",
      "reads read      : 53,408,292\n",
      "reads written   : 53,408,292\n",
      "spots read      : 17,580,775\n",
      "reads read      : 35,161,550\n",
      "reads written   : 35,161,550\n",
      "spots read      : 13,008,763\n",
      "reads read      : 26,017,526\n",
      "reads written   : 26,017,526\n"
     ]
    }
   ],
   "source": [
    "!for x in `cat accs.txt`; do fasterq-dump -f -O data/raw_fastq -e $CORES -m 4G data/raw_fastq/$x/$x.sra; done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert to fastq.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pigz data/raw_fastq/*.fastq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### STEP 4: Copy reference transcriptome files that will be used by Salmon using E-Direct\n",
    "\n",
    "We will need a refseq assembly here. Try searching through [NCBI genome database](https://www.ncbi.nlm.nih.gov/datasets/genome) for the appropriate Plasmodium falciparum assembly.\n",
    "\n",
    "Salmon is a tool that aligns RNA-Seq reads to a transcriptome.\n",
    "\n",
    "So we will need a transcriptome reference file.\n",
    "\n",
    "To get one, we can search through the NCBI assembly database, find an assembly, and download transcriptome reference files from that assembly using FTP links.\n",
    "\n",
    "For instance, we will use the <a href='https://www.ncbi.nlm.nih.gov/assembly/GCF_001632805.1'>ASM163280v1</a> refseq assembly, found by searching through the NCBI assembly database. The FTP links can be accessed through the website in various ways, one way is to click the 'FTP directory for RefSeq assembly' link, found under 'Access the data', section.\n",
    "\n",
    "Alternatively, if one were inclined, one could take the less common route and perform this through the NCBI command line tool suite called 'Entrez Direct' (EDirect).\n",
    "\n",
    "This is an intricate and complicated set of tools, with many ways to do any one thing.\n",
    "\n",
    "Below is an example of using an eDirect search query with a refseq identifier to obtain the relevant FTP directory, and then using that to download desired reference files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 6504k  100 6504k    0     0  7443k      0 --:--:-- --:--:-- --:--:-- 7442k\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  995k  100  995k    0     0  1490k      0 --:--:-- --:--:-- --:--:-- 1492k\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  320k  100  320k    0     0   382k      0 --:--:-- --:--:-- --:--:--  383k\n"
     ]
    }
   ],
   "source": [
    "#parse for the ftp link and download the genome reference fasta file\n",
    "\n",
    "!esearch -db assembly -query GCF_000002765.6 | efetch -format docsum \\\n",
    "| xtract -pattern DocumentSummary -element FtpPath_RefSeq \\\n",
    "| awk -F\"/\" '{print \"curl -o data/reference/\"$NF\"_genomic.fna.gz \" $0\"/\"$NF\"_genomic.fna.gz\"}' \\\n",
    "| bash\n",
    "\n",
    "#parse for the ftp link and download the gtf reference fasta file\n",
    "\n",
    "!esearch -db assembly -query GCF_000002765.6 | efetch -format docsum \\\n",
    "| xtract -pattern DocumentSummary -element FtpPath_RefSeq \\\n",
    "| awk -F\"/\" '{print \"curl -o data/reference/\"$NF\"_genomic.gff.gz \" $0\"/\"$NF\"_genomic.gff.gz\"}' \\\n",
    "| bash\n",
    "\n",
    "# parse for the ftp link and download the feature-table reference file \n",
    "# (for later use for merging readcounts with gene names in R code).\n",
    "\n",
    "!esearch -db assembly -query GCF_000002765.6 | efetch -format docsum \\\n",
    "| xtract -pattern DocumentSummary -element FtpPath_RefSeq \\\n",
    "| awk -F\"/\" '{print \"curl -o data/reference/\"$NF\"_feature_table.txt.gz \" $0\"/\"$NF\"_feature_table.txt.gz\"}' \\\n",
    "| bash\n",
    "\n",
    "\n",
    "#unzip the compresseed fasta files\n",
    "\n",
    "!gzip -d data/reference/*.gz --force"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we can use a tool called gffread to create a transcriptome reference file using the gtf and genome files we downloaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FASTA index file data/reference/GCF_000002765.6_GCA_000002765_genomic.fna.fai created.\n"
     ]
    }
   ],
   "source": [
    "!gffread -w data/reference/GCF_000002765.6_transcriptome_reference.fa -g data/reference/GCF_000002765.6_GCA_000002765_genomic.fna data/reference/GCF_000002765.6_GCA_000002765_genomic.gff\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is also recommended to include the full genome at the end of the transcriptome reference file, for the purpose of performing a 'decoy-aware' mapping, more information about which can be found in the Salmon documentation.\n",
    "\n",
    "To alert the tool to the presence of this, we will also create a 'decoy file', which salmon needs pointed towards the full genome sequence in our transcriptome reference file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat data/reference/GCF_000002765.6_transcriptome_reference.fa <(echo) data/reference/GCF_000002765.6_GCA_000002765_genomic.fna > data/reference/GCF_000002765.6_transcriptome_reference_w_decoy.fa\n",
    "\n",
    "!cat data/reference/GCF_000002765.6_GCA_000002765_genomic.fna | grep \">\" | sed 's/Plasmodium.*//g' | sed 's/>//g' > data/reference/decoys.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 5: Copy data file for Trimmomatic\n",
    "\n",
    "One of trimmomatics functions is to trim sequence machine specific adapter sequences. These are usually within the trimmomatic installation directory in a folder called adapters.\n",
    "\n",
    "Directories of packages within conda installations can be confusing, so in the case of using conda with trimmomatic, it may be easier to simply download or create a file with the relevant adapter sequencecs and store it in an easy to find directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying gs://nigms-sandbox/me-inbre-rnaseq-pipelinev2/config/TruSeq3-PE.fa...\n",
      "/ [1/1 files][   95.0 B/   95.0 B] 100% Done                                    \n",
      "Operation completed over 1 objects/95.0 B.                                       \n",
      ">PrefixPE/1\n",
      "TACACTCTTTCCCTACACGACGCTCTTCCGATCT\n",
      ">PrefixPE/2\n",
      "GTGACTGGAGTTCAGACGTGTGCTCTTCCGATCT\n",
      "\n"
     ]
    }
   ],
   "source": [
    "! gsutil -m cp -r gs://nigms-sandbox/me-inbre-rnaseq-pipelinev2/config/TruSeq3-PE.fa .\n",
    "!head TruSeq3-PE.fa "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 6: Run Trimmomatic\n",
    "Trimmomatic will trim off any adapter sequences or low quality sequence it detects in the FASTQ files.\n",
    "\n",
    "Using piping and our original list, it is possible to queue up a batch run of trimmomatic for all our files, note that this is a different way to run a loop compared with what we did before.\n",
    "\n",
    "The below code may take approximately 35 minutes to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrimmomaticPE: Started with arguments:\n",
      " -threads 16 data/raw_fastq/SRR11784387_1.fastq.gz data/raw_fastq/SRR11784387_2.fastq.gz data/trimmed/SRR11784387_1_trimmed.fastq.gz data/trimmed/SRR11784387_1_trimmed_unpaired.fastq.gz data/trimmed/SRR11784387_2_trimmed.fastq.gz data/trimmed/SRR11784387_2_trimmed_unpaired.fastq.gz ILLUMINACLIP:TruSeq3-PE.fa:2:30:10:2:keepBothReads LEADING:3 TRAILING:3 MINLEN:36\n",
      "Using PrefixPair: 'TACACTCTTTCCCTACACGACGCTCTTCCGATCT' and 'GTGACTGGAGTTCAGACGTGTGCTCTTCCGATCT'\n",
      "ILLUMINACLIP: Using 1 prefix pairs, 0 forward/reverse sequences, 0 forward only sequences, 0 reverse only sequences\n",
      "Quality encoding detected as phred33\n",
      "Input Read Pairs: 13008763 Both Surviving: 12877289 (98.99%) Forward Only Surviving: 131474 (1.01%) Reverse Only Surviving: 0 (0.00%) Dropped: 0 (0.00%)\n",
      "TrimmomaticPE: Completed successfully\n",
      "TrimmomaticPE: Started with arguments:\n",
      " -threads 16 data/raw_fastq/SRR11784410_1.fastq.gz data/raw_fastq/SRR11784410_2.fastq.gz data/trimmed/SRR11784410_1_trimmed.fastq.gz data/trimmed/SRR11784410_1_trimmed_unpaired.fastq.gz data/trimmed/SRR11784410_2_trimmed.fastq.gz data/trimmed/SRR11784410_2_trimmed_unpaired.fastq.gz ILLUMINACLIP:TruSeq3-PE.fa:2:30:10:2:keepBothReads LEADING:3 TRAILING:3 MINLEN:36\n",
      "Using PrefixPair: 'TACACTCTTTCCCTACACGACGCTCTTCCGATCT' and 'GTGACTGGAGTTCAGACGTGTGCTCTTCCGATCT'\n",
      "ILLUMINACLIP: Using 1 prefix pairs, 0 forward/reverse sequences, 0 forward only sequences, 0 reverse only sequences\n",
      "Quality encoding detected as phred33\n",
      "Input Read Pairs: 26704146 Both Surviving: 26387720 (98.82%) Forward Only Surviving: 316426 (1.18%) Reverse Only Surviving: 0 (0.00%) Dropped: 0 (0.00%)\n",
      "TrimmomaticPE: Completed successfully\n",
      "TrimmomaticPE: Started with arguments:\n",
      " -threads 16 data/raw_fastq/SRR11784398_1.fastq.gz data/raw_fastq/SRR11784398_2.fastq.gz data/trimmed/SRR11784398_1_trimmed.fastq.gz data/trimmed/SRR11784398_1_trimmed_unpaired.fastq.gz data/trimmed/SRR11784398_2_trimmed.fastq.gz data/trimmed/SRR11784398_2_trimmed_unpaired.fastq.gz ILLUMINACLIP:TruSeq3-PE.fa:2:30:10:2:keepBothReads LEADING:3 TRAILING:3 MINLEN:36\n",
      "Using PrefixPair: 'TACACTCTTTCCCTACACGACGCTCTTCCGATCT' and 'GTGACTGGAGTTCAGACGTGTGCTCTTCCGATCT'\n",
      "ILLUMINACLIP: Using 1 prefix pairs, 0 forward/reverse sequences, 0 forward only sequences, 0 reverse only sequences\n",
      "Quality encoding detected as phred33\n",
      "Input Read Pairs: 17580775 Both Surviving: 17273212 (98.25%) Forward Only Surviving: 307563 (1.75%) Reverse Only Surviving: 0 (0.00%) Dropped: 0 (0.00%)\n",
      "TrimmomaticPE: Completed successfully\n"
     ]
    }
   ],
   "source": [
    "!cat accs.txt | xargs -I {} trimmomatic PE -threads $CORES 'data/raw_fastq/{}_1.fastq.gz' 'data/raw_fastq/{}_2.fastq.gz' 'data/trimmed/{}_1_trimmed.fastq.gz' 'data/trimmed/{}_1_trimmed_unpaired.fastq.gz' 'data/trimmed/{}_2_trimmed.fastq.gz' 'data/trimmed/{}_2_trimmed_unpaired.fastq.gz' ILLUMINACLIP:TruSeq3-PE.fa:2:30:10:2:keepBothReads LEADING:3 TRAILING:3 MINLEN:36"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 7: Run FastQC\n",
    "FastQC is an invaluable tool that allows you to evaluate whether there are problems with a set of reads. For example, it will provide a report of whether there is any bias in the sequence composition of the reads.\n",
    "\n",
    "If you notice the results of the trimming, you may have noted the sequences in the reverse reads were few, and largely unpaired. This may be an artifact from how the original sequencing process. This is okay, we can proceed from here simply using the forward reads.\n",
    "\n",
    "The below code may take around 10 minutes to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "application/gzip\n",
      "Started analysis of SRR11784387_1_trimmed.fastq.gz\n",
      "Approx 5% complete for SRR11784387_1_trimmed.fastq.gz\n",
      "Approx 10% complete for SRR11784387_1_trimmed.fastq.gz\n",
      "Approx 15% complete for SRR11784387_1_trimmed.fastq.gz\n",
      "Approx 20% complete for SRR11784387_1_trimmed.fastq.gz\n",
      "Approx 25% complete for SRR11784387_1_trimmed.fastq.gz\n",
      "Approx 30% complete for SRR11784387_1_trimmed.fastq.gz\n",
      "Approx 35% complete for SRR11784387_1_trimmed.fastq.gz\n",
      "Approx 40% complete for SRR11784387_1_trimmed.fastq.gz\n",
      "Approx 45% complete for SRR11784387_1_trimmed.fastq.gz\n",
      "Approx 50% complete for SRR11784387_1_trimmed.fastq.gz\n",
      "Approx 55% complete for SRR11784387_1_trimmed.fastq.gz\n",
      "Approx 60% complete for SRR11784387_1_trimmed.fastq.gz\n",
      "Approx 65% complete for SRR11784387_1_trimmed.fastq.gz\n",
      "Approx 70% complete for SRR11784387_1_trimmed.fastq.gz\n",
      "Approx 75% complete for SRR11784387_1_trimmed.fastq.gz\n",
      "Approx 80% complete for SRR11784387_1_trimmed.fastq.gz\n",
      "Approx 85% complete for SRR11784387_1_trimmed.fastq.gz\n",
      "Approx 90% complete for SRR11784387_1_trimmed.fastq.gz\n",
      "Approx 95% complete for SRR11784387_1_trimmed.fastq.gz\n",
      "Analysis complete for SRR11784387_1_trimmed.fastq.gz\n",
      "application/gzip\n",
      "Started analysis of SRR11784410_1_trimmed.fastq.gz\n",
      "Approx 5% complete for SRR11784410_1_trimmed.fastq.gz\n",
      "Approx 10% complete for SRR11784410_1_trimmed.fastq.gz\n",
      "Approx 15% complete for SRR11784410_1_trimmed.fastq.gz\n",
      "Approx 20% complete for SRR11784410_1_trimmed.fastq.gz\n",
      "Approx 25% complete for SRR11784410_1_trimmed.fastq.gz\n",
      "Approx 30% complete for SRR11784410_1_trimmed.fastq.gz\n",
      "Approx 35% complete for SRR11784410_1_trimmed.fastq.gz\n",
      "Approx 40% complete for SRR11784410_1_trimmed.fastq.gz\n",
      "Approx 45% complete for SRR11784410_1_trimmed.fastq.gz\n",
      "Approx 50% complete for SRR11784410_1_trimmed.fastq.gz\n",
      "Approx 55% complete for SRR11784410_1_trimmed.fastq.gz\n",
      "Approx 60% complete for SRR11784410_1_trimmed.fastq.gz\n",
      "Approx 65% complete for SRR11784410_1_trimmed.fastq.gz\n",
      "Approx 70% complete for SRR11784410_1_trimmed.fastq.gz\n",
      "Approx 75% complete for SRR11784410_1_trimmed.fastq.gz\n",
      "Approx 80% complete for SRR11784410_1_trimmed.fastq.gz\n",
      "Approx 85% complete for SRR11784410_1_trimmed.fastq.gz\n",
      "Approx 90% complete for SRR11784410_1_trimmed.fastq.gz\n",
      "Approx 95% complete for SRR11784410_1_trimmed.fastq.gz\n",
      "Analysis complete for SRR11784410_1_trimmed.fastq.gz\n",
      "application/gzip\n",
      "Started analysis of SRR11784398_1_trimmed.fastq.gz\n",
      "Approx 5% complete for SRR11784398_1_trimmed.fastq.gz\n",
      "Approx 10% complete for SRR11784398_1_trimmed.fastq.gz\n",
      "Approx 15% complete for SRR11784398_1_trimmed.fastq.gz\n",
      "Approx 20% complete for SRR11784398_1_trimmed.fastq.gz\n",
      "Approx 25% complete for SRR11784398_1_trimmed.fastq.gz\n",
      "Approx 30% complete for SRR11784398_1_trimmed.fastq.gz\n",
      "Approx 35% complete for SRR11784398_1_trimmed.fastq.gz\n",
      "Approx 40% complete for SRR11784398_1_trimmed.fastq.gz\n",
      "Approx 45% complete for SRR11784398_1_trimmed.fastq.gz\n",
      "Approx 50% complete for SRR11784398_1_trimmed.fastq.gz\n",
      "Approx 55% complete for SRR11784398_1_trimmed.fastq.gz\n",
      "Approx 60% complete for SRR11784398_1_trimmed.fastq.gz\n",
      "Approx 65% complete for SRR11784398_1_trimmed.fastq.gz\n",
      "Approx 70% complete for SRR11784398_1_trimmed.fastq.gz\n",
      "Approx 75% complete for SRR11784398_1_trimmed.fastq.gz\n",
      "Approx 80% complete for SRR11784398_1_trimmed.fastq.gz\n",
      "Approx 85% complete for SRR11784398_1_trimmed.fastq.gz\n",
      "Approx 90% complete for SRR11784398_1_trimmed.fastq.gz\n",
      "Approx 95% complete for SRR11784398_1_trimmed.fastq.gz\n",
      "Analysis complete for SRR11784398_1_trimmed.fastq.gz\n"
     ]
    }
   ],
   "source": [
    "!cat accs.txt | xargs -I {} fastqc -t $CORES \"data/trimmed/{}_1_trimmed.fastq.gz\" -o data/fastqc/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 8: Run MultiQC\n",
    "MultiQC reads in the FastQC reports and generate a compiled report for all the analyzed FASTQ files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  \u001b[91m///\u001b[0m \u001b]8;id=131870;https://multiqc.info\u001b\\\u001b[1mMultiQC\u001b[0m\u001b]8;;\u001b\\ üîç \u001b[2m| v1.17\u001b[0m\n",
      "\n",
      "\u001b[34m|           multiqc\u001b[0m | Search path : /home/jupyter/RNA-Seq-Differential-Expression-Analysis/data/fastqc\n",
      "\u001b[2K\u001b[34m|\u001b[0m         \u001b[34msearching\u001b[0m | \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[35m100%\u001b[0m \u001b[32m12/12\u001b[0m  12\u001b[0m  \n",
      "\u001b[?25h\u001b[34m|            fastqc\u001b[0m | Found 6 reports\n",
      "\u001b[34m|           multiqc\u001b[0m | Report      : multiqc_report.html\n",
      "\u001b[34m|           multiqc\u001b[0m | Data        : multiqc_data   (overwritten)\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/bin/multiqc\", line 10, in <module>\n",
      "    sys.exit(run_multiqc())\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/multiqc/__main__.py\", line 23, in run_multiqc\n",
      "    multiqc.run_cli(prog_name=\"multiqc\")\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/click/core.py\", line 1130, in __call__\n",
      "    return self.main(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/rich_click/rich_command.py\", line 126, in main\n",
      "    rv = self.invoke(ctx)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/click/core.py\", line 1404, in invoke\n",
      "    return ctx.invoke(self.callback, **ctx.params)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/click/core.py\", line 760, in invoke\n",
      "    return __callback(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/multiqc/multiqc.py\", line 298, in run_cli\n",
      "    multiqc_run = run(**kwargs)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/multiqc/multiqc.py\", line 1073, in run\n",
      "    shutil.copytree(template_mod.template_dir, tmp_dir, dirs_exist_ok=True)\n",
      "TypeError: copytree() got an unexpected keyword argument 'dirs_exist_ok'\n"
     ]
    }
   ],
   "source": [
    "!multiqc -f data/fastqc/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 9: Index the Transcriptome so that Trimmed Reads Can Be Mapped Using Salmon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Version Server Response: Not Found\n",
      "[2023-12-15 20:56:42.421] [jLog] [info] building index\n",
      "out : data/reference/transcriptome_index\n",
      "\u001b[00m[2023-12-15 20:56:42.421] [puff::index::jointLog] [info] Running fixFasta\n",
      "\u001b[00m\n",
      "[Step 1 of 4] : counting k-mers\n",
      "\n",
      "\u001b[35m[2023-12-15 20:56:43.146] [puff::index::jointLog] [warning] There were 17 transcripts that would need to be removed to avoid duplicates.\n",
      "\u001b[00m\u001b[00m[2023-12-15 20:56:43.146] [puff::index::jointLog] [info] Replaced 0 non-ATCG nucleotides\n",
      "\u001b[00m\u001b[00m[2023-12-15 20:56:43.146] [puff::index::jointLog] [info] Clipped poly-A tails from 1 transcripts\n",
      "\u001b[00mwrote 5543 cleaned references\n",
      "\u001b[00m[2023-12-15 20:56:43.191] [puff::index::jointLog] [info] Filter size not provided; estimating from number of distinct k-mers\n",
      "\u001b[00m\u001b[00m[2023-12-15 20:56:43.531] [puff::index::jointLog] [info] ntHll estimated 21261541 distinct k-mers, setting filter size to 2^29\n",
      "\u001b[00mThreads = 16\n",
      "Vertex length = 31\n",
      "Hash functions = 5\n",
      "Filter size = 536870912\n",
      "Capacity = 2\n",
      "Files: \n",
      "data/reference/transcriptome_index/ref_k31_fixed.fa\n",
      "--------------------------------------------------------------------------------\n",
      "Round 0, 0:536870912\n",
      "Pass\tFilling\tFiltering\n",
      "1\t2\t1\t\n",
      "2\t2\t0\n",
      "True junctions count = 168382\n",
      "False junctions count = 282428\n",
      "Hash table size = 450810\n",
      "Candidate marks count = 1755935\n",
      "--------------------------------------------------------------------------------\n",
      "Reallocating bifurcations time: 0\n",
      "True marks count: 1310986\n",
      "Edges construction time: 2\n",
      "--------------------------------------------------------------------------------\n",
      "Distinct junctions = 168382\n",
      "\n",
      "TwoPaCo::buildGraphMain:: allocated with scalable_malloc; freeing.\n",
      "TwoPaCo::buildGraphMain:: Calling scalable_allocation_command(TBBMALLOC_CLEAN_ALL_BUFFERS, 0);\n",
      "allowedIn: 18\n",
      "Max Junction ID: 168736\n",
      "seen.size():1349897 kmerInfo.size():168737\n",
      "approximateContigTotalLength: 13201046\n",
      "counters for complex kmers:\n",
      "(prec>1 & succ>1)=34732 | (succ>1 & isStart)=8 | (prec>1 & isEnd)=12 | (isStart & isEnd)=0\n",
      "contig count: 273038 element count: 29622858 complex nodes: 34752\n",
      "# of ones in rank vector: 273037\n",
      "\u001b[00m[2023-12-15 20:56:53.437] [puff::index::jointLog] [info] Starting the Pufferfish indexing by reading the GFA binary file.\n",
      "\u001b[00m\u001b[00m[2023-12-15 20:56:53.437] [puff::index::jointLog] [info] Setting the index/BinaryGfa directory data/reference/transcriptome_index\n",
      "\u001b[00msize = 29622858\n",
      "-----------------------------------------\n",
      "| Loading contigs | Time = 4.8466 ms\n",
      "-----------------------------------------\n",
      "size = 29622858\n",
      "-----------------------------------------\n",
      "| Loading contig boundaries | Time = 2.4547 ms\n",
      "-----------------------------------------\n",
      "Number of ones: 273037\n",
      "Number of ones per inventory item: 512\n",
      "Inventory entries filled: 534\n",
      "273037\n",
      "\u001b[00m[2023-12-15 20:56:53.516] [puff::index::jointLog] [info] Done wrapping the rank vector with a rank9sel structure.\n",
      "\u001b[00m\u001b[00m[2023-12-15 20:56:53.522] [puff::index::jointLog] [info] contig count for validation: 273037\n",
      "\u001b[00m\u001b[00m[2023-12-15 20:56:53.602] [puff::index::jointLog] [info] Total # of Contigs : 273037\n",
      "\u001b[00m\u001b[00m[2023-12-15 20:56:53.602] [puff::index::jointLog] [info] Total # of numerical Contigs : 273037\n",
      "\u001b[00m\u001b[00m[2023-12-15 20:56:53.606] [puff::index::jointLog] [info] Total # of contig vec entries: 1388002\n",
      "\u001b[00m\u001b[00m[2023-12-15 20:56:53.606] [puff::index::jointLog] [info] bits per offset entry 21\n",
      "\u001b[00m\u001b[00m[2023-12-15 20:56:53.626] [puff::index::jointLog] [info] Done constructing the contig vector. 273038\n",
      "\u001b[00m\u001b[00m[2023-12-15 20:56:53.769] [puff::index::jointLog] [info] # segments = 273037\n",
      "\u001b[00m\u001b[00m[2023-12-15 20:56:53.769] [puff::index::jointLog] [info] total length = 29622858\n",
      "\u001b[00m\u001b[00m[2023-12-15 20:56:53.772] [puff::index::jointLog] [info] Reading the reference files ...\n",
      "\u001b[00m\u001b[00m[2023-12-15 20:56:53.888] [puff::index::jointLog] [info] positional integer width = 25\n",
      "\u001b[00m\u001b[00m[2023-12-15 20:56:53.888] [puff::index::jointLog] [info] seqSize = 29622858\n",
      "\u001b[00m\u001b[00m[2023-12-15 20:56:53.888] [puff::index::jointLog] [info] rankSize = 29622858\n",
      "\u001b[00m\u001b[00m[2023-12-15 20:56:53.888] [puff::index::jointLog] [info] edgeVecSize = 0\n",
      "\u001b[00m\u001b[00m[2023-12-15 20:56:53.888] [puff::index::jointLog] [info] num keys = 21431748\n",
      "\u001b[00mfor info, total work write each  : 2.331    total work inram from level 3 : 4.322  total work raw : 25.000 \n",
      "[Building BooPHF]  100  %   elapsed:   0 min 1  sec   remaining:   0 min 0  sec\n",
      "Bitarray       112301376  bits (100.00 %)   (array + ranks )\n",
      "final hash             0  bits (0.00 %) (nb in final hash 0)\n",
      "\u001b[00m[2023-12-15 20:56:54.553] [puff::index::jointLog] [info] mphf size = 13.3874 MB\n",
      "\u001b[00m\u001b[00m[2023-12-15 20:56:54.567] [puff::index::jointLog] [info] chunk size = 1851429\n",
      "\u001b[00m\u001b[00m[2023-12-15 20:56:54.567] [puff::index::jointLog] [info] chunk 0 = [0, 1851440)\n",
      "\u001b[00m\u001b[00m[2023-12-15 20:56:54.567] [puff::index::jointLog] [info] chunk 1 = [1851440, 3702869)\n",
      "\u001b[00m\u001b[00m[2023-12-15 20:56:54.567] [puff::index::jointLog] [info] chunk 2 = [3702869, 5554298)\n",
      "\u001b[00m\u001b[00m[2023-12-15 20:56:54.567] [puff::index::jointLog] [info] chunk 3 = [5554298, 7405727)\n",
      "\u001b[00m\u001b[00m[2023-12-15 20:56:54.567] [puff::index::jointLog] [info] chunk 4 = [7405727, 9257156)\n",
      "\u001b[00m\u001b[00m[2023-12-15 20:56:54.567] [puff::index::jointLog] [info] chunk 5 = [9257156, 11108585)\n",
      "\u001b[00m\u001b[00m[2023-12-15 20:56:54.567] [puff::index::jointLog] [info] chunk 6 = [11108585, 12960014)\n",
      "\u001b[00m\u001b[00m[2023-12-15 20:56:54.567] [puff::index::jointLog] [info] chunk 7 = [12960014, 14811459)\n",
      "\u001b[00m\u001b[00m[2023-12-15 20:56:54.567] [puff::index::jointLog] [info] chunk 8 = [14811459, 16662888)\n",
      "\u001b[00m\u001b[00m[2023-12-15 20:56:54.567] [puff::index::jointLog] [info] chunk 9 = [16662888, 18514317)\n",
      "\u001b[00m\u001b[00m[2023-12-15 20:56:54.567] [puff::index::jointLog] [info] chunk 10 = [18514317, 20365757)\n",
      "\u001b[00m\u001b[00m[2023-12-15 20:56:54.567] [puff::index::jointLog] [info] chunk 11 = [20365757, 22217205)\n",
      "\u001b[00m\u001b[00m[2023-12-15 20:56:54.567] [puff::index::jointLog] [info] chunk 12 = [22217205, 24068646)\n",
      "\u001b[00m\u001b[00m[2023-12-15 20:56:54.567] [puff::index::jointLog] [info] chunk 13 = [24068646, 25920075)\n",
      "\u001b[00m\u001b[00m[2023-12-15 20:56:54.567] [puff::index::jointLog] [info] chunk 14 = [25920075, 27771504)\n",
      "\u001b[00m\u001b[00m[2023-12-15 20:56:54.567] [puff::index::jointLog] [info] chunk 15 = [27771504, 29622828)\n",
      "\u001b[00m\u001b[00m[2023-12-15 20:56:54.851] [puff::index::jointLog] [info] finished populating pos vector\n",
      "\u001b[00m\u001b[00m[2023-12-15 20:56:54.851] [puff::index::jointLog] [info] writing index components\n",
      "\u001b[00m\u001b[00m[2023-12-15 20:56:55.356] [puff::index::jointLog] [info] finished writing dense pufferfish index\n",
      "\u001b[00m[2023-12-15 20:56:55.364] [jLog] [info] done building index\n"
     ]
    }
   ],
   "source": [
    "!salmon index -t data/reference/GCF_000002765.6_transcriptome_reference_w_decoy.fa -p $CORES -i data/reference/transcriptome_index --decoys data/reference/decoys.txt -k 31 --keepDuplicates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 10: Run Salmon to Map Reads to Transcripts and Quantify Expression Levels\n",
    "Salmon aligns the trimmed reads to the reference transcriptome and generates the read counts per transcript. In this analysis, each gene has a single transcript."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Version Server Response: Not Found\n",
      "### salmon (selective-alignment-based) v1.10.2\n",
      "### [ program ] => salmon \n",
      "### [ command ] => quant \n",
      "### [ index ] => { data/reference/transcriptome_index }\n",
      "### [ libType ] => { SR }\n",
      "### [ unmatedReads ] => { data/trimmed/SRR11784387_1_trimmed.fastq.gz }\n",
      "### [ threads ] => { 16 }\n",
      "### [ validateMappings ] => { }\n",
      "### [ output ] => { data/quants/SRR11784387_quant }\n",
      "Logs will be written to data/quants/SRR11784387_quant/logs\n",
      "\u001b[00m[2023-12-15 20:57:04.541] [jointLog] [info] setting maxHashResizeThreads to 16\n",
      "\u001b[00m\u001b[00m[2023-12-15 20:57:04.541] [jointLog] [info] Fragment incompatibility prior below threshold.  Incompatible fragments will be ignored.\n",
      "\u001b[00m\u001b[00m[2023-12-15 20:57:04.541] [jointLog] [info] Usage of --validateMappings implies use of minScoreFraction. Since not explicitly specified, it is being set to 0.65\n",
      "\u001b[00m\u001b[00m[2023-12-15 20:57:04.541] [jointLog] [info] Setting consensusSlack to selective-alignment default of 0.35.\n",
      "\u001b[00m\u001b[00m[2023-12-15 20:57:04.541] [jointLog] [info] parsing read library format\n",
      "\u001b[00m\u001b[00m[2023-12-15 20:57:04.541] [jointLog] [info] There is 1 library.\n",
      "\u001b[00m\u001b[00m[2023-12-15 20:57:04.541] [jointLog] [info] Loading pufferfish index\n",
      "\u001b[00m\u001b[00m[2023-12-15 20:57:04.541] [jointLog] [info] Loading dense pufferfish index.\n",
      "\u001b[00m-----------------------------------------\n",
      "| Loading contig table | Time = 102.52 ms\n",
      "-----------------------------------------\n",
      "size = 273038\n",
      "-----------------------------------------\n",
      "| Loading contig offsets | Time = 560.87 us\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| Loading reference lengths | Time = 25.629 us\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| Loading mphf table | Time = 7.1627 ms\n",
      "-----------------------------------------\n",
      "size = 29622858\n",
      "Number of ones: 273037\n",
      "Number of ones per inventory item: 512\n",
      "Inventory entries filled: 534\n",
      "-----------------------------------------\n",
      "| Loading contig boundaries | Time = 71.51 ms\n",
      "-----------------------------------------\n",
      "size = 29622858\n",
      "-----------------------------------------\n",
      "| Loading sequence | Time = 5.2199 ms\n",
      "-----------------------------------------\n",
      "size = 21431748\n",
      "-----------------------------------------\n",
      "| Loading positions | Time = 43.523 ms\n",
      "-----------------------------------------\n",
      "size = 35698695\n",
      "-----------------------------------------\n",
      "| Loading reference sequence | Time = 5.5433 ms\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| Loading reference accumulative lengths | Time = 38.905 us\n",
      "-----------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[00m[2023-12-15 20:57:04.778] [jointLog] [info] done\n",
      "\u001b[00m\u001b[00m[2023-12-15 20:57:04.814] [jointLog] [info] Index contained 5543 targets\n",
      "\u001b[00m\u001b[00m[2023-12-15 20:57:04.815] [jointLog] [info] Number of decoys : 14\n",
      "\u001b[00m\u001b[00m[2023-12-15 20:57:04.815] [jointLog] [info] First decoy index : 5529 \n",
      "\u001b[32mprocessed\u001b[31m 500000 \u001b[32mfragments\u001b[0m\n",
      "\u001b[32mprocessed\u001b[31m 1000000 \u001b[32mfragments\u001b[0m\n",
      "\u001b[32mprocessed\u001b[31m 1500000 \u001b[32mfragments\u001b[0m\n",
      "\u001b[32mprocessed\u001b[31m 2000000 \u001b[32mfragments\u001b[0m\n",
      "\u001b[32mprocessed\u001b[31m 2500000 \u001b[32mfragments\u001b[0m\n",
      "\u001b[32mprocessed\u001b[31m 3000000 \u001b[32mfragments\u001b[0m\n",
      "\u001b[32mprocessed\u001b[31m 3500000 \u001b[32mfragments\u001b[0m\n",
      "\u001b[32mprocessed\u001b[31m 4000000 \u001b[32mfragments\u001b[0m\n",
      "\u001b[32mprocessed\u001b[31m 4500000 \u001b[32mfragments\u001b[0m\n",
      "\u001b[32mprocessed\u001b[31m 5000000 \u001b[32mfragments\u001b[0m\n",
      "\u001b[32mprocessed\u001b[31m 5500000 \u001b[32mfragments\u001b[0m\n",
      "\u001b[32mprocessed\u001b[31m 6000000 \u001b[32mfragments\u001b[0m\n",
      "\u001b[32mprocessed\u001b[31m 6500000 \u001b[32mfragments\u001b[0m\n",
      "\u001b[32mprocessed\u001b[31m 7000001 \u001b[32mfragments\u001b[0m\n",
      "\u001b[32mprocessed\u001b[31m 7500000 \u001b[32mfragments\u001b[0m\n",
      "\u001b[32mprocessed\u001b[31m 8000000 \u001b[32mfragments\u001b[0m\n",
      "\u001b[32mprocessed\u001b[31m 8500000 \u001b[32mfragments\u001b[0m\n",
      "\u001b[32mprocessed\u001b[31m 9000000 \u001b[32mfragments\u001b[0m\n",
      "\u001b[32mprocessed\u001b[31m 9500000 \u001b[32mfragments\u001b[0m\n",
      "\u001b[32mprocessed\u001b[31m 10000000 \u001b[32mfragments\u001b[0m\n",
      "\u001b[32mprocessed\u001b[31m 10500000 \u001b[32mfragments\u001b[0m\n",
      "\u001b[32mprocessed\u001b[31m 11000000 \u001b[32mfragments\u001b[0m\n",
      "\u001b[32mprocessed\u001b[31m 11500000 \u001b[32mfragments\u001b[0m\n",
      "\u001b[32mprocessed\u001b[31m 12000000 \u001b[32mfragments\u001b[0m\n",
      "\u001b[32mprocessed\u001b[31m 12500000 \u001b[32mfragments\u001b[0m\n",
      "hits: 11651769; hits per frag:  0.936886\u001b[00m[2023-12-15 20:57:54.322] [jointLog] [info] Thread saw mini-batch with a maximum of 35.32% zero probability fragments\n",
      "\u001b[00m\u001b[00m[2023-12-15 20:57:54.341] [jointLog] [info] Thread saw mini-batch with a maximum of 35.18% zero probability fragments\n",
      "\u001b[00m\u001b[00m[2023-12-15 20:57:54.357] [jointLog] [info] Thread saw mini-batch with a maximum of 35.36% zero probability fragments\n",
      "\u001b[00m\u001b[00m[2023-12-15 20:57:54.374] [jointLog] [info] Thread saw mini-batch with a maximum of 35.24% zero probability fragments\n",
      "\u001b[00m\u001b[00m[2023-12-15 20:57:54.410] [jointLog] [info] Thread saw mini-batch with a maximum of 35.56% zero probability fragments\n",
      "\u001b[00m\u001b[00m[2023-12-15 20:57:54.418] [jointLog] [info] Thread saw mini-batch with a maximum of 34.88% zero probability fragments\n",
      "\u001b[00m\u001b[00m[2023-12-15 20:57:54.432] [jointLog] [info] Thread saw mini-batch with a maximum of 36.06% zero probability fragments\n",
      "\u001b[00m\u001b[00m[2023-12-15 20:57:54.433] [jointLog] [info] Thread saw mini-batch with a maximum of 35.28% zero probability fragments\n",
      "\u001b[00m\u001b[00m[2023-12-15 20:57:54.441] [jointLog] [info] Thread saw mini-batch with a maximum of 34.98% zero probability fragments\n",
      "\u001b[00m\u001b[00m[2023-12-15 20:57:54.455] [jointLog] [info] Thread saw mini-batch with a maximum of 35.40% zero probability fragments\n",
      "\u001b[00m\u001b[00m[2023-12-15 20:57:54.460] [jointLog] [info] Thread saw mini-batch with a maximum of 35.08% zero probability fragments\n",
      "\u001b[00m\u001b[00m[2023-12-15 20:57:54.495] [jointLog] [info] Thread saw mini-batch with a maximum of 35.38% zero probability fragments\n",
      "\u001b[00m\u001b[00m[2023-12-15 20:57:54.514] [jointLog] [info] Thread saw mini-batch with a maximum of 35.14% zero probability fragments\n",
      "\u001b[00m\u001b[00m[2023-12-15 20:57:54.530] [jointLog] [info] Thread saw mini-batch with a maximum of 35.40% zero probability fragments\n",
      "\u001b[00m\u001b[00m[2023-12-15 20:57:54.547] [jointLog] [info] Thread saw mini-batch with a maximum of 35.12% zero probability fragments\n",
      "\u001b[00m\u001b[00m[2023-12-15 20:57:54.569] [jointLog] [info] Thread saw mini-batch with a maximum of 35.34% zero probability fragments\n",
      "\u001b[00m\n",
      "\n",
      "\n",
      "\n",
      "\u001b[00m[2023-12-15 20:57:54.610] [jointLog] [info] Computed 5251 rich equivalence classes for further processing\n",
      "\u001b[00m\u001b[00m[2023-12-15 20:57:54.610] [jointLog] [info] Counted 4261986 total reads in the equivalence classes \n",
      "\u001b[00m\n",
      "\n",
      "\n",
      "\n",
      "\u001b[00m[2023-12-15 20:57:54.618] [jointLog] [info] Number of mappings discarded because of alignment score : 23860627\n",
      "\u001b[00m\u001b[00m[2023-12-15 20:57:54.618] [jointLog] [info] Number of fragments entirely discarded because of alignment score : 1434308\n",
      "\u001b[00m\u001b[00m[2023-12-15 20:57:54.618] [jointLog] [info] Number of fragments discarded because they are best-mapped to decoys : 626739\n",
      "\u001b[00m\u001b[00m[2023-12-15 20:57:54.618] [jointLog] [info] Number of fragments discarded because they have only dovetail (discordant) mappings to valid targets : 0\n",
      "\u001b[00m\u001b[35m[2023-12-15 20:57:54.619] [jointLog] [warning] Only 4261986 fragments were mapped, but the number of burn-in fragments was set to 5000000.\n",
      "The effective lengths have been computed using the observed mappings.\n",
      "\n",
      "\u001b[00m\u001b[00m[2023-12-15 20:57:54.619] [jointLog] [info] Mapping rate = 33.0969%\n",
      "\n",
      "\u001b[00m\u001b[00m[2023-12-15 20:57:54.619] [jointLog] [info] finished quantifyLibrary()\n",
      "\u001b[00m\u001b[00m[2023-12-15 20:57:54.619] [jointLog] [info] Starting optimizer\n",
      "\u001b[00m\u001b[00m[2023-12-15 20:57:54.622] [jointLog] [info] Marked 0 weighted equivalence classes as degenerate\n",
      "\u001b[00m\u001b[00m[2023-12-15 20:57:54.624] [jointLog] [info] iteration = 0 | max rel diff. = 1428.59\n",
      "\u001b[00m\u001b[00m[2023-12-15 20:57:54.650] [jointLog] [info] iteration = 100 | max rel diff. = 0.0569085\n",
      "\u001b[00m\u001b[00m[2023-12-15 20:57:54.666] [jointLog] [info] iteration = 168 | max rel diff. = 0.00380524\n",
      "\u001b[00m\u001b[00m[2023-12-15 20:57:54.666] [jointLog] [info] Finished optimizer\n",
      "\u001b[00m\u001b[00m[2023-12-15 20:57:54.666] [jointLog] [info] writing output \n",
      "\n",
      "\u001b[00mVersion Server Response: Not Found\n",
      "### salmon (selective-alignment-based) v1.10.2\n",
      "### [ program ] => salmon \n",
      "### [ command ] => quant \n",
      "### [ index ] => { data/reference/transcriptome_index }\n",
      "### [ libType ] => { SR }\n",
      "### [ unmatedReads ] => { data/trimmed/SRR11784410_1_trimmed.fastq.gz }\n",
      "### [ threads ] => { 16 }\n",
      "### [ validateMappings ] => { }\n",
      "### [ output ] => { data/quants/SRR11784410_quant }\n",
      "Logs will be written to data/quants/SRR11784410_quant/logs\n",
      "\u001b[00m[2023-12-15 20:57:54.849] [jointLog] [info] setting maxHashResizeThreads to 16\n",
      "\u001b[00m\u001b[00m[2023-12-15 20:57:54.849] [jointLog] [info] Fragment incompatibility prior below threshold.  Incompatible fragments will be ignored.\n",
      "\u001b[00m\u001b[00m[2023-12-15 20:57:54.849] [jointLog] [info] Usage of --validateMappings implies use of minScoreFraction. Since not explicitly specified, it is being set to 0.65\n",
      "\u001b[00m\u001b[00m[2023-12-15 20:57:54.849] [jointLog] [info] Setting consensusSlack to selective-alignment default of 0.35.\n",
      "\u001b[00m\u001b[00m[2023-12-15 20:57:54.849] [jointLog] [info] parsing read library format\n",
      "\u001b[00m\u001b[00m[2023-12-15 20:57:54.849] [jointLog] [info] There is 1 library.\n",
      "\u001b[00m\u001b[00m[2023-12-15 20:57:54.849] [jointLog] [info] Loading pufferfish index\n",
      "\u001b[00m\u001b[00m[2023-12-15 20:57:54.849] [jointLog] [info] Loading dense pufferfish index.\n",
      "\u001b[00m-----------------------------------------\n",
      "| Loading contig table | Time = 95.629 ms\n",
      "-----------------------------------------\n",
      "size = 273038\n",
      "-----------------------------------------\n",
      "| Loading contig offsets | Time = 538.29 us\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| Loading reference lengths | Time = 24.468 us\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| Loading mphf table | Time = 6.9664 ms\n",
      "-----------------------------------------\n",
      "size = 29622858\n",
      "Number of ones: 273037\n",
      "Number of ones per inventory item: 512\n",
      "Inventory entries filled: 534\n",
      "-----------------------------------------\n",
      "| Loading contig boundaries | Time = 71.221 ms\n",
      "-----------------------------------------\n",
      "size = 29622858\n",
      "-----------------------------------------\n",
      "| Loading sequence | Time = 5.0582 ms\n",
      "-----------------------------------------\n",
      "size = 21431748\n",
      "-----------------------------------------\n",
      "| Loading positions | Time = 43.115 ms\n",
      "-----------------------------------------\n",
      "size = 35698695\n",
      "-----------------------------------------\n",
      "| Loading reference sequence | Time = 5.4369 ms\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| Loading reference accumulative lengths | Time = 34.649 us\n",
      "-----------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[00m[2023-12-15 20:57:55.077] [jointLog] [info] done\n",
      "\u001b[00m\u001b[00m[2023-12-15 20:57:55.115] [jointLog] [info] Index contained 5543 targets\n",
      "\u001b[00m\u001b[00m[2023-12-15 20:57:55.116] [jointLog] [info] Number of decoys : 14\n",
      "\u001b[00m\u001b[00m[2023-12-15 20:57:55.116] [jointLog] [info] First decoy index : 5529 \n",
      "\u001b[32mprocessed\u001b[31m 500000 \u001b[32mfragments\u001b[0m\n",
      "\u001b[32mprocessed\u001b[31m 1000000 \u001b[32mfragments\u001b[0m\n",
      "\u001b[32mprocessed\u001b[31m 1500000 \u001b[32mfragments\u001b[0m\n",
      "\u001b[32mprocessed\u001b[31m 2000000 \u001b[32mfragments\u001b[0m\n",
      "\u001b[32mprocessed\u001b[31m 2500000 \u001b[32mfragments\u001b[0m\n",
      "\u001b[32mprocessed\u001b[31m 3000000 \u001b[32mfragments\u001b[0m\n",
      "\u001b[32mprocessed\u001b[31m 3500000 \u001b[32mfragments\u001b[0m\n",
      "\u001b[32mprocessed\u001b[31m 4000000 \u001b[32mfragments\u001b[0m\n",
      "\u001b[32mprocessed\u001b[31m 4500000 \u001b[32mfragments\u001b[0m\n",
      "\u001b[32mprocessed\u001b[31m 5000000 \u001b[32mfragments\u001b[0m\n",
      "\u001b[32mprocessed\u001b[31m 5500000 \u001b[32mfragments\u001b[0m\n",
      "\u001b[32mprocessed\u001b[31m 6000000 \u001b[32mfragments\u001b[0m\n",
      "\u001b[32mprocessed\u001b[31m 6500000 \u001b[32mfragments\u001b[0m\n",
      "\u001b[32mprocessed\u001b[31m 7000000 \u001b[32mfragments\u001b[0m\n",
      "\u001b[32mprocessed\u001b[31m 7500000 \u001b[32mfragments\u001b[0m\n",
      "\u001b[32mprocessed\u001b[31m 8000000 \u001b[32mfragments\u001b[0m\n",
      "\u001b[32mprocessed\u001b[31m 8500000 \u001b[32mfragments\u001b[0m\n",
      "\u001b[32mprocessed\u001b[31m 9000000 \u001b[32mfragments\u001b[0m\n",
      "\u001b[32mprocessed\u001b[31m 9500000 \u001b[32mfragments\u001b[0m\n",
      "\u001b[32mprocessed\u001b[31m 10000000 \u001b[32mfragments\u001b[0m\n",
      "\u001b[32mprocessed\u001b[31m 10500000 \u001b[32mfragments\u001b[0m\n",
      "\u001b[32mprocessed\u001b[31m 11000001 \u001b[32mfragments\u001b[0m\n",
      "\u001b[32mprocessed\u001b[31m 11500000 \u001b[32mfragments\u001b[0m\n",
      "\u001b[32mprocessed\u001b[31m 12000000 \u001b[32mfragments\u001b[0m\n",
      "\u001b[32mprocessed\u001b[31m 12500001 \u001b[32mfragments\u001b[0m\n",
      "\u001b[32mprocessed\u001b[31m 13000000 \u001b[32mfragments\u001b[0m\n",
      "\u001b[32mprocessed\u001b[31m 13500000 \u001b[32mfragments\u001b[0m\n",
      "\u001b[32mprocessed\u001b[31m 14000000 \u001b[32mfragments\u001b[0m\n",
      "\u001b[32mprocessed\u001b[31m 14500000 \u001b[32mfragments\u001b[0m\n",
      "\u001b[32mprocessed\u001b[31m 15000000 \u001b[32mfragments\u001b[0m\n",
      "\u001b[32mprocessed\u001b[31m 15500000 \u001b[32mfragments\u001b[0m\n",
      "\u001b[32mprocessed\u001b[31m 16000001 \u001b[32mfragments\u001b[0m\n",
      "\u001b[32mprocessed\u001b[31m 16500000 \u001b[32mfragments\u001b[0m\n",
      "\u001b[32mprocessed\u001b[31m 17000000 \u001b[32mfragments\u001b[0m\n",
      "\u001b[32mprocessed\u001b[31m 17500000 \u001b[32mfragments\u001b[0m\n",
      "\u001b[32mprocessed\u001b[31m 18000000 \u001b[32mfragments\u001b[0m\n",
      "\u001b[32mprocessed\u001b[31m 18500000 \u001b[32mfragments\u001b[0m\n",
      "\u001b[32mprocessed\u001b[31m 19000000 \u001b[32mfragments\u001b[0m\n",
      "\u001b[32mprocessed\u001b[31m 19500000 \u001b[32mfragments\u001b[0m\n",
      "\u001b[32mprocessed\u001b[31m 20000001 \u001b[32mfragments\u001b[0m\n",
      "\u001b[32mprocessed\u001b[31m 20500000 \u001b[32mfragments\u001b[0m\n",
      "\u001b[32mprocessed\u001b[31m 21000000 \u001b[32mfragments\u001b[0m\n",
      "\u001b[32mprocessed\u001b[31m 21500000 \u001b[32mfragments\u001b[0m\n",
      "\u001b[32mprocessed\u001b[31m 22000000 \u001b[32mfragments\u001b[0m\n",
      "\u001b[32mprocessed\u001b[31m 22500000 \u001b[32mfragments\u001b[0m\n",
      "\u001b[32mprocessed\u001b[31m 23000000 \u001b[32mfragments\u001b[0m\n",
      "\u001b[32mprocessed\u001b[31m 23500001 \u001b[32mfragments\u001b[0m\n",
      "\u001b[32mprocessed\u001b[31m 24000000 \u001b[32mfragments\u001b[0m\n",
      "\u001b[32mprocessed\u001b[31m 24500000 \u001b[32mfragments\u001b[0m\n",
      "\u001b[32mprocessed\u001b[31m 25000000 \u001b[32mfragments\u001b[0m\n",
      "\u001b[32mprocessed\u001b[31m 25500000 \u001b[32mfragments\u001b[0m\n",
      "\u001b[32mprocessed\u001b[31m 26000000 \u001b[32mfragments\u001b[0m\n",
      "hits: 23156848; hits per frag:  0.89262\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[00m[2023-12-15 20:59:19.929] [jointLog] [info] Thread saw mini-batch with a maximum of 39.46% zero probability fragments\n",
      "\u001b[00m\u001b[00m[2023-12-15 20:59:19.930] [jointLog] [info] Thread saw mini-batch with a maximum of 39.58% zero probability fragments\n",
      "\u001b[00m\u001b[00m[2023-12-15 20:59:19.935] [jointLog] [info] Thread saw mini-batch with a maximum of 39.32% zero probability fragments\n",
      "\u001b[00m\u001b[00m[2023-12-15 20:59:19.953] [jointLog] [info] Thread saw mini-batch with a maximum of 39.90% zero probability fragments\n",
      "\u001b[00m\u001b[00m[2023-12-15 20:59:19.963] [jointLog] [info] Thread saw mini-batch with a maximum of 40.48% zero probability fragments\n",
      "\u001b[00m\u001b[00m[2023-12-15 20:59:19.982] [jointLog] [info] Thread saw mini-batch with a maximum of 39.80% zero probability fragments\n",
      "\u001b[00m\u001b[00m[2023-12-15 20:59:19.982] [jointLog] [info] Thread saw mini-batch with a maximum of 39.78% zero probability fragments\n",
      "\u001b[00m\u001b[00m[2023-12-15 20:59:20.024] [jointLog] [info] Thread saw mini-batch with a maximum of 39.84% zero probability fragments\n",
      "\u001b[00m\u001b[00m[2023-12-15 20:59:20.029] [jointLog] [info] Thread saw mini-batch with a maximum of 40.50% zero probability fragments\n",
      "\u001b[00m\u001b[00m[2023-12-15 20:59:20.035] [jointLog] [info] Thread saw mini-batch with a maximum of 40.08% zero probability fragments\n",
      "\u001b[00m\u001b[00m[2023-12-15 20:59:20.036] [jointLog] [info] Thread saw mini-batch with a maximum of 39.68% zero probability fragments\n",
      "\u001b[00m\u001b[00m[2023-12-15 20:59:20.040] [jointLog] [info] Thread saw mini-batch with a maximum of 39.98% zero probability fragments\n",
      "\u001b[00m\u001b[00m[2023-12-15 20:59:20.053] [jointLog] [info] Thread saw mini-batch with a maximum of 39.68% zero probability fragments\n",
      "\u001b[00m\u001b[00m[2023-12-15 20:59:20.058] [jointLog] [info] Thread saw mini-batch with a maximum of 39.72% zero probability fragments\n",
      "\u001b[00m\u001b[00m[2023-12-15 20:59:20.106] [jointLog] [info] Thread saw mini-batch with a maximum of 39.52% zero probability fragments\n",
      "\u001b[00m\u001b[00m[2023-12-15 20:59:20.110] [jointLog] [info] Thread saw mini-batch with a maximum of 39.40% zero probability fragments\n",
      "\u001b[00m\u001b[00m[2023-12-15 20:59:20.150] [jointLog] [info] Computed 5565 rich equivalence classes for further processing\n",
      "\u001b[00m\u001b[00m[2023-12-15 20:59:20.150] [jointLog] [info] Counted 9875942 total reads in the equivalence classes \n",
      "\u001b[00m\u001b[00m[2023-12-15 20:59:20.158] [jointLog] [info] Number of mappings discarded because of alignment score : 34369070\n",
      "\u001b[00m\u001b[00m[2023-12-15 20:59:20.158] [jointLog] [info] Number of fragments entirely discarded because of alignment score : 3746088\n",
      "\u001b[00m\u001b[00m[2023-12-15 20:59:20.158] [jointLog] [info] Number of fragments discarded because they are best-mapped to decoys : 1945120\n",
      "\u001b[00m\u001b[00m[2023-12-15 20:59:20.158] [jointLog] [info] Number of fragments discarded because they have only dovetail (discordant) mappings to valid targets : 0\n",
      "\u001b[00m\u001b[00m[2023-12-15 20:59:20.158] [jointLog] [info] Mapping rate = 37.4263%\n",
      "\n",
      "\u001b[00m\u001b[00m[2023-12-15 20:59:20.158] [jointLog] [info] finished quantifyLibrary()\n",
      "\u001b[00m\u001b[00m[2023-12-15 20:59:20.158] [jointLog] [info] Starting optimizer\n",
      "\u001b[00m\u001b[00m[2023-12-15 20:59:20.162] [jointLog] [info] Marked 0 weighted equivalence classes as degenerate\n",
      "\u001b[00m\u001b[00m[2023-12-15 20:59:20.162] [jointLog] [info] iteration = 0 | max rel diff. = 2868.18\n",
      "\u001b[00m\u001b[00m[2023-12-15 20:59:20.187] [jointLog] [info] iteration = 100 | max rel diff. = 0.0171887\n",
      "\u001b[00m\u001b[00m[2023-12-15 20:59:20.212] [jointLog] [info] iteration = 200 | max rel diff. = 0.127356\n",
      "\u001b[00m\u001b[00m[2023-12-15 20:59:20.214] [jointLog] [info] iteration = 210 | max rel diff. = 0.00926276\n",
      "\u001b[00m\u001b[00m[2023-12-15 20:59:20.215] [jointLog] [info] Finished optimizer\n",
      "\u001b[00m\u001b[00m[2023-12-15 20:59:20.215] [jointLog] [info] writing output \n",
      "\n",
      "\u001b[00mVersion Server Response: Not Found\n",
      "### salmon (selective-alignment-based) v1.10.2\n",
      "### [ program ] => salmon \n",
      "### [ command ] => quant \n",
      "### [ index ] => { data/reference/transcriptome_index }\n",
      "### [ libType ] => { SR }\n",
      "### [ unmatedReads ] => { data/trimmed/SRR11784398_1_trimmed.fastq.gz }\n",
      "### [ threads ] => { 16 }\n",
      "### [ validateMappings ] => { }\n",
      "### [ output ] => { data/quants/SRR11784398_quant }\n",
      "Logs will be written to data/quants/SRR11784398_quant/logs\n",
      "\u001b[00m[2023-12-15 20:59:21.043] [jointLog] [info] setting maxHashResizeThreads to 16\n",
      "\u001b[00m\u001b[00m[2023-12-15 20:59:21.043] [jointLog] [info] Fragment incompatibility prior below threshold.  Incompatible fragments will be ignored.\n",
      "\u001b[00m\u001b[00m[2023-12-15 20:59:21.043] [jointLog] [info] Usage of --validateMappings implies use of minScoreFraction. Since not explicitly specified, it is being set to 0.65\n",
      "\u001b[00m\u001b[00m[2023-12-15 20:59:21.043] [jointLog] [info] Setting consensusSlack to selective-alignment default of 0.35.\n",
      "\u001b[00m\u001b[00m[2023-12-15 20:59:21.043] [jointLog] [info] parsing read library format\n",
      "\u001b[00m\u001b[00m[2023-12-15 20:59:21.043] [jointLog] [info] There is 1 library.\n",
      "\u001b[00m\u001b[00m[2023-12-15 20:59:21.043] [jointLog] [info] Loading pufferfish index\n",
      "\u001b[00m\u001b[00m[2023-12-15 20:59:21.043] [jointLog] [info] Loading dense pufferfish index.\n",
      "\u001b[00m-----------------------------------------\n",
      "| Loading contig table | Time = 95.394 ms\n",
      "-----------------------------------------\n",
      "size = 273038\n",
      "-----------------------------------------\n",
      "| Loading contig offsets | Time = 538.19 us\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| Loading reference lengths | Time = 21.326 us\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| Loading mphf table | Time = 6.4541 ms\n",
      "-----------------------------------------\n",
      "size = 29622858\n",
      "Number of ones: 273037\n",
      "Number of ones per inventory item: 512\n",
      "Inventory entries filled: 534\n",
      "-----------------------------------------\n",
      "| Loading contig boundaries | Time = 71.546 ms\n",
      "-----------------------------------------\n",
      "size = 29622858\n",
      "-----------------------------------------\n",
      "| Loading sequence | Time = 5.1276 ms\n",
      "-----------------------------------------\n",
      "size = 21431748\n",
      "-----------------------------------------\n",
      "| Loading positions | Time = 43.755 ms\n",
      "-----------------------------------------\n",
      "size = 35698695\n",
      "-----------------------------------------\n",
      "| Loading reference sequence | Time = 5.2511 ms\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| Loading reference accumulative lengths | Time = 28.948 us\n",
      "-----------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[00m[2023-12-15 20:59:21.271] [jointLog] [info] done\n",
      "\u001b[00m\u001b[00m[2023-12-15 20:59:21.307] [jointLog] [info] Index contained 5543 targets\n",
      "\u001b[00m\u001b[00m[2023-12-15 20:59:21.308] [jointLog] [info] Number of decoys : 14\n",
      "\u001b[00m\u001b[00m[2023-12-15 20:59:21.308] [jointLog] [info] First decoy index : 5529 \n",
      "\u001b[32mprocessed\u001b[31m 500000 \u001b[32mfragments\u001b[0m\n",
      "\u001b[32mprocessed\u001b[31m 1000000 \u001b[32mfragments\u001b[0m\n",
      "\u001b[32mprocessed\u001b[31m 1500000 \u001b[32mfragments\u001b[0m\n",
      "\u001b[32mprocessed\u001b[31m 2000000 \u001b[32mfragments\u001b[0m\n",
      "\u001b[32mprocessed\u001b[31m 2500000 \u001b[32mfragments\u001b[0m\n",
      "\u001b[32mprocessed\u001b[31m 3000002 \u001b[32mfragments\u001b[0m\n",
      "\u001b[32mprocessed\u001b[31m 3500000 \u001b[32mfragments\u001b[0m\n",
      "\u001b[32mprocessed\u001b[31m 4000000 \u001b[32mfragments\u001b[0m\n",
      "\u001b[32mprocessed\u001b[31m 4500000 \u001b[32mfragments\u001b[0m\n",
      "\u001b[32mprocessed\u001b[31m 5000001 \u001b[32mfragments\u001b[0m\n",
      "\u001b[32mprocessed\u001b[31m 5500000 \u001b[32mfragments\u001b[0m\n",
      "\u001b[32mprocessed\u001b[31m 6000000 \u001b[32mfragments\u001b[0m\n",
      "\u001b[32mprocessed\u001b[31m 6500000 \u001b[32mfragments\u001b[0m\n",
      "\u001b[32mprocessed\u001b[31m 7000000 \u001b[32mfragments\u001b[0m\n",
      "\u001b[32mprocessed\u001b[31m 7500000 \u001b[32mfragments\u001b[0m\n",
      "\u001b[32mprocessed\u001b[31m 8000000 \u001b[32mfragments\u001b[0m\n",
      "\u001b[32mprocessed\u001b[31m 8500000 \u001b[32mfragments\u001b[0m\n",
      "\u001b[32mprocessed\u001b[31m 9000000 \u001b[32mfragments\u001b[0m\n",
      "\u001b[32mprocessed\u001b[31m 9500000 \u001b[32mfragments\u001b[0m\n",
      "\u001b[32mprocessed\u001b[31m 10000000 \u001b[32mfragments\u001b[0m\n",
      "\u001b[32mprocessed\u001b[31m 10500000 \u001b[32mfragments\u001b[0m\n",
      "\u001b[32mprocessed\u001b[31m 11000000 \u001b[32mfragments\u001b[0m\n",
      "\u001b[32mprocessed\u001b[31m 11500000 \u001b[32mfragments\u001b[0m\n",
      "\u001b[32mprocessed\u001b[31m 12000000 \u001b[32mfragments\u001b[0m\n",
      "\u001b[32mprocessed\u001b[31m 12500000 \u001b[32mfragments\u001b[0m\n",
      "\u001b[32mprocessed\u001b[31m 13000000 \u001b[32mfragments\u001b[0m\n",
      "\u001b[32mprocessed\u001b[31m 13500000 \u001b[32mfragments\u001b[0m\n",
      "\u001b[32mprocessed\u001b[31m 14000000 \u001b[32mfragments\u001b[0m\n",
      "\u001b[32mprocessed\u001b[31m 14500000 \u001b[32mfragments\u001b[0m\n",
      "\u001b[32mprocessed\u001b[31m 15000000 \u001b[32mfragments\u001b[0m\n",
      "\u001b[32mprocessed\u001b[31m 15500000 \u001b[32mfragments\u001b[0m\n",
      "\u001b[32mprocessed\u001b[31m 16000000 \u001b[32mfragments\u001b[0m\n",
      "\u001b[32mprocessed\u001b[31m 16500000 \u001b[32mfragments\u001b[0m\n",
      "\u001b[32mprocessed\u001b[31m 17000000 \u001b[32mfragments\u001b[0m\n",
      "hits: 14346231; hits per frag:  0.847371\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[00m[2023-12-15 21:00:20.720] [jointLog] [info] Thread saw mini-batch with a maximum of 38.82% zero probability fragments\n",
      "\u001b[00m\u001b[00m[2023-12-15 21:00:20.729] [jointLog] [info] Thread saw mini-batch with a maximum of 39.00% zero probability fragments\n",
      "\u001b[00m\u001b[00m[2023-12-15 21:00:20.743] [jointLog] [info] Thread saw mini-batch with a maximum of 38.92% zero probability fragments\n",
      "\u001b[00m\u001b[00m[2023-12-15 21:00:20.761] [jointLog] [info] Thread saw mini-batch with a maximum of 38.30% zero probability fragments\n",
      "\u001b[00m\u001b[00m[2023-12-15 21:00:20.765] [jointLog] [info] Thread saw mini-batch with a maximum of 38.48% zero probability fragments\n",
      "\u001b[00m\u001b[00m[2023-12-15 21:00:20.768] [jointLog] [info] Thread saw mini-batch with a maximum of 38.40% zero probability fragments\n",
      "\u001b[00m\u001b[00m[2023-12-15 21:00:20.824] [jointLog] [info] Thread saw mini-batch with a maximum of 39.04% zero probability fragments\n",
      "\u001b[00m\u001b[00m[2023-12-15 21:00:20.829] [jointLog] [info] Thread saw mini-batch with a maximum of 39.64% zero probability fragments\n",
      "\u001b[00m\u001b[00m[2023-12-15 21:00:20.854] [jointLog] [info] Thread saw mini-batch with a maximum of 39.00% zero probability fragments\n",
      "\u001b[00m\u001b[00m[2023-12-15 21:00:20.864] [jointLog] [info] Thread saw mini-batch with a maximum of 39.00% zero probability fragments\n",
      "\u001b[00m\u001b[00m[2023-12-15 21:00:20.866] [jointLog] [info] Thread saw mini-batch with a maximum of 38.66% zero probability fragments\n",
      "\u001b[00m\u001b[00m[2023-12-15 21:00:20.870] [jointLog] [info] Thread saw mini-batch with a maximum of 38.82% zero probability fragments\n",
      "\u001b[00m\u001b[00m[2023-12-15 21:00:20.871] [jointLog] [info] Thread saw mini-batch with a maximum of 39.20% zero probability fragments\n",
      "\u001b[00m\u001b[00m[2023-12-15 21:00:20.871] [jointLog] [info] Thread saw mini-batch with a maximum of 38.48% zero probability fragments\n",
      "\u001b[00m\u001b[00m[2023-12-15 21:00:20.892] [jointLog] [info] Thread saw mini-batch with a maximum of 38.60% zero probability fragments\n",
      "\u001b[00m\u001b[00m[2023-12-15 21:00:20.897] [jointLog] [info] Thread saw mini-batch with a maximum of 38.70% zero probability fragments\n",
      "\u001b[00m\u001b[00m[2023-12-15 21:00:20.936] [jointLog] [info] Computed 5492 rich equivalence classes for further processing\n",
      "\u001b[00m\u001b[00m[2023-12-15 21:00:20.936] [jointLog] [info] Counted 6286113 total reads in the equivalence classes \n",
      "\u001b[00m\u001b[00m[2023-12-15 21:00:20.944] [jointLog] [info] Number of mappings discarded because of alignment score : 29712440\n",
      "\u001b[00m\u001b[00m[2023-12-15 21:00:20.944] [jointLog] [info] Number of fragments entirely discarded because of alignment score : 3172444\n",
      "\u001b[00m\u001b[00m[2023-12-15 21:00:20.944] [jointLog] [info] Number of fragments discarded because they are best-mapped to decoys : 1535249\n",
      "\u001b[00m\u001b[00m[2023-12-15 21:00:20.944] [jointLog] [info] Number of fragments discarded because they have only dovetail (discordant) mappings to valid targets : 0\n",
      "\u001b[00m\u001b[00m[2023-12-15 21:00:20.944] [jointLog] [info] Mapping rate = 36.3923%\n",
      "\n",
      "\u001b[00m\u001b[00m[2023-12-15 21:00:20.944] [jointLog] [info] finished quantifyLibrary()\n",
      "\u001b[00m\u001b[00m[2023-12-15 21:00:20.944] [jointLog] [info] Starting optimizer\n",
      "\u001b[00m\u001b[00m[2023-12-15 21:00:20.948] [jointLog] [info] Marked 0 weighted equivalence classes as degenerate\n",
      "\u001b[00m\u001b[00m[2023-12-15 21:00:20.948] [jointLog] [info] iteration = 0 | max rel diff. = 1987.25\n",
      "\u001b[00m\u001b[00m[2023-12-15 21:00:20.973] [jointLog] [info] iteration = 100 | max rel diff. = 0.086923\n",
      "\u001b[00m\u001b[00m[2023-12-15 21:00:20.995] [jointLog] [info] iteration = 184 | max rel diff. = 0.00914127\n",
      "\u001b[00m\u001b[00m[2023-12-15 21:00:20.995] [jointLog] [info] Finished optimizer\n",
      "\u001b[00m\u001b[00m[2023-12-15 21:00:20.995] [jointLog] [info] writing output \n",
      "\n",
      "\u001b[00m"
     ]
    }
   ],
   "source": [
    "!cat accs.txt | xargs -I {} salmon quant -i data/reference/transcriptome_index -l SR -r \"data/trimmed/{}_1_trimmed.fastq.gz\" -p $CORES --validateMappings -o \"data/quants/{}_quant\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 11: Report the top 10 most highly expressed genes in the samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Top 10 most highly expressed genes in each wild-type sample.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name\tLength\tEffectiveLength\tTPM\tNumReads\n",
      "rna-XM_001347701.1\t312\t62.441\t62647.794533\t1999.000\n",
      "rna-XM_001348736.3\t1014\t764.000\t45909.700034\t17924.000\n",
      "rna-XM_961070.1\t399\t149.000\t27514.437041\t2095.000\n",
      "rna-XM_001349953.1\t951\t701.000\t23354.077683\t8366.000\n",
      "rna-XR_002273103.1\t159\t5.631\t17897.187939\t51.500\n",
      "rna-XR_002273083.1\t159\t5.631\t17897.187939\t51.500\n",
      "rna-XR_002273104.1\t199\t8.782\t17591.080307\t78.940\n",
      "rna-XM_002808709.1\t165\t5.972\t16383.649843\t50.000\n",
      "rna-XR_002273085.2\t3788\t3538.000\t16106.393946\t29120.127\n",
      "rna-XM_001347370.1\t696\t446.000\t15602.341142\t3556.000\n",
      "sort: write failed: 'standard output': Broken pipe\n",
      "sort: write error\n",
      "rna-XM_001347680.1\t285\t38.947\t159170.317502\t3528.000\n",
      "rna-XM_001349503.1\t321\t71.167\t98883.838238\t4005.000\n",
      "rna-XM_002808697.2\t918\t668.000\t35730.020232\t13583.324\n",
      "rna-XM_001348563.3\t1110\t860.000\t26653.195632\t13045.000\n",
      "rna-XM_001347701.1\t312\t62.441\t24116.517322\t857.000\n",
      "rna-XR_002273085.2\t3788\t3538.000\t22670.427542\t45647.173\n",
      "rna-XM_001347268.1\t324\t74.118\t18847.259208\t795.000\n",
      "rna-XM_002809070.1\t750\t500.000\t18639.624723\t5304.000\n",
      "rna-XM_002809080.1\t828\t578.000\t18038.466809\t5933.676\n",
      "rna-XR_002966665.1\t242\t16.957\t17822.592292\t172.000\n",
      "sort: write failed: 'standard output': Broken pipe\n",
      "sort: write error\n",
      "rna-XM_001347680.1\t285\t38.947\t122247.434748\t1480.000\n",
      "rna-XR_002273085.2\t3788\t3538.000\t75736.966809\t83294.735\n",
      "rna-XM_001349503.1\t321\t71.167\t70109.864212\t1551.000\n",
      "rna-XR_002273105.1\t3790\t3540.000\t51693.636480\t56884.266\n",
      "rna-XR_002273081.2\t2087\t1837.000\t48102.163019\t27467.908\n",
      "rna-XM_002808697.2\t918\t668.000\t38969.308371\t8091.909\n",
      "rna-XR_002273101.1\t2092\t1842.000\t26996.912424\t15458.080\n",
      "rna-XM_002809080.1\t828\t578.000\t16669.801112\t2995.091\n",
      "rna-XM_001347268.1\t324\t74.118\t16276.344584\t375.000\n",
      "rna-XM_001348736.3\t1014\t764.000\t16046.997269\t3811.000\n",
      "sort: write failed: 'standard output': Broken pipe\n",
      "sort: write error\n"
     ]
    }
   ],
   "source": [
    "!head data/quants/SRR11784387_quant/quant.sf -n 1\n",
    "!sort -nrk 4,4 data/quants/SRR11784398_quant/quant.sf | head -10\n",
    "!sort -nrk 4,4 data/quants/SRR11784410_quant/quant.sf | head -10\n",
    "!sort -nrk 4,4 data/quants/SRR11784387_quant//quant.sf | head -10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Top 10 most highly expressed genes in the double lysogen samples.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head data/quants/SRR13349122_quant/quant.sf -n 1\n",
    "!sort -nrk 4,4 data/quants/SRR13349128_quant/quant.sf | head -10\n",
    "!sort -nrk 4,4 data/quants/SRR13349129_quant/quant.sf | head -10\n",
    "!sort -nrk 4,4 data/quants/SRR13349130_quant/quant.sf | head -10\n",
    "!sort -nrk 4,4 data/quants/SRR13349131_quant/quant.sf | head -10\n",
    "!sort -nrk 4,4 data/quants/SRR13349132_quant/quant.sf | head -10\n",
    "!sort -nrk 4,4 data/quants/SRR13349133_quant/quant.sf | head -10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 12: Report the expression of a putative acyl-ACP desaturase (BB28_RS16545) that was downregulated in the double lysogen relative to wild-type\n",
    "A acyl-transferase was reported to be downregulated in the double lysogen as shown in the table of the top 20 upregulated and downregulated genes from the paper describing the study.\n",
    "![RNA-Seq workflow](images/table-cushman.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use `grep` to report the expression in the wild-type sample. The fields in the Salmon `quant.sf` file are as follows. The level of expression is reported in the Transcripts Per Million (`TPM`) and number of reads (`NumReads`) fields:  \n",
    "`Name    Length  EffectiveLength TPM     NumReads`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!grep 'BB28_RS16545' data/quants/SRR13349122_quant/quant.sf\n",
    "!grep 'BB28_RS16545' data/quants/SRR13349123_quant/quant.sf\n",
    "!grep 'BB28_RS16545' data/quants/SRR13349124_quant/quant.sf\n",
    "!grep 'BB28_RS16545' data/quants/SRR13349125_quant/quant.sf\n",
    "!grep 'BB28_RS16545' data/quants/SRR13349126_quant/quant.sf\n",
    "!grep 'BB28_RS16545' data/quants/SRR13349127_quant/quant.sf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use `grep` to report the expression in the double lysogen sample. The fields in the Salmon `quant.sf` file are as follows. The level of expression is reported in the Transcripts Per Million (`TPM`) and number of reads (`NumReads`) fields:  \n",
    "`Name    Length  EffectiveLength TPM     NumReads`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!grep 'BB28_RS16545' data/quants/SRR13349128_quant/quant.sf\n",
    "!grep 'BB28_RS16545' data/quants/SRR13349129_quant/quant.sf\n",
    "!grep 'BB28_RS16545' data/quants/SRR13349130_quant/quant.sf\n",
    "!grep 'BB28_RS16545' data/quants/SRR13349131_quant/quant.sf\n",
    "!grep 'BB28_RS16545' data/quants/SRR13349132_quant/quant.sf\n",
    "!grep 'BB28_RS16545' data/quants/SRR13349133_quant/quant.sf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 12: Combine Genecounts to a Single Genecount File\n",
    "Commonly, the readcounts for each sample are combined into a single table, where the rows contain the gene ID, and the columns identify the sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##first merge salmon files by number of reads.\n",
    "!salmon quantmerge --column numreads --quants data/quants/*_quant -o data/quants/merged_quants.txt\n",
    "##optinally we can rename the columns\n",
    "!sed -i \"1s/.*/Name\\tSRR13349122\\tSRR13349123\\tSRR13349124\\tSRR13349125\\tSRR13349126\\tSRR13349127\\tSRR13349128\\tSRR13349129\\tSRR13349130\\tSRR13349131\\tSRR13349132\\tSRR13349133/\" data/quants/merged_quants.txt\n",
    "\n",
    "##for further formatting, it may be easier in our r-code to later merge\n",
    "##if we remove the gene- and rna- prefix\n",
    "!sed -i \"s/gene-//\" data/quants/merged_quants.txt\n",
    "!sed -i \"s/rna-//\" data/quants/merged_quants.txt\n",
    "\n",
    "print(\"An example of a combined genecount outputfile.\")\n",
    "!head data/quants/merged_quants.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## <a name=\"workflow\">Additional Workflows</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have read counts per gene, feel free to explore the R workflow which creates plots and analyses using these readcount files, or try other alternate workflows for creating read count files, such as using snakemake."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "[Workflow One:](Tutorial_1.ipynb) A short introduction to downloading and mapping sequences to a transcriptome using Trimmomatic and Salmon. Here is a link to the YouTube video demonstrating the tutorial: <https://youtu.be/ChGfBR4do_Y>.\n",
    "\n",
    "[Workflow One (Extended):](Tutorial_1B_Extended.ipynb) An extended version of workflow one. Once you have got your feet wet, you can retry workflow one with this extended version that covers the entire dataset, and includes elaboration such as using SRA tools for sequence downloading, and examples of running batches of fastq files through the pipeline. This workflow may take around an hour to run.\n",
    "\n",
    "[Workflow One (Using Snakemake):](Tutorial_2_Snakemake.ipynb) Using snakemake to run workflow one.\n",
    "\n",
    "[Workflow Two (DEG Analysis):](Tutorial_3_DEG_Analysis.ipynb) Using Deseq2 and R to conduct clustering and differential gene expression analysis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![RNA-Seq workflow](images/RNA-Seq_Notebook_Homepage.png)"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": ".m112",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/:m112"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
