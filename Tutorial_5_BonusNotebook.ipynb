{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Extended RNA-Seq Analysis Training Demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, we will repeat the Tutorial 1B extended submodule with a different dataset. Using the provided dataset is a great way to get introduced to the module and learn its basic functionality, but to use it with your own data or to understand the module at a deeper level, it is helpful to practice adapting it to a new dataset. We will guide you through that here, providing basic guidance on the workflow and letting you write the commands yourself. If you get stuck, you can always view our hints and suggestions by clicking on the dropdown arrow under the command cells.\n",
    "\n",
    "The initial dataset used a prokaryotic sample for simplicity and compute efficiency. This time, we will increase the complexity a bit by using a eukaryotic sample. We have selected data from a time series experiment on Plasmodium falciparum, the parasite responsible for malaria. The experiment did RNA-seq analysis on P. falciparum cells at 25 time points after erythrocyte invasion. The data is taken from Kucharski M, Tripathi J, Nayak S, Zhu L et al. A comprehensive RNA handling and transcriptomics guide for high-throughput processing of Plasmodium blood-stage samples. Malar J 2020 Oct 9;19(1):363. The sequence data is available from SRA with the accession number [SRP261441](https://www.ncbi.nlm.nih.gov/sra/?term=SRP261441). To keep things simple, we are not using all time points, and have selected time points 1, 13, and 25 as a proxy for early, mid, and late infection. Feel free to add or remove samples from your analysis to see how the results differ. The workflow structure remains the same as the original submodule 1B, with the diagram shown below.\n",
    "\n",
    "![RNA-Seq workflow](images/rnaseq-workflow.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 1: Install Mambaforge and then install snakemake using bioconda."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First install Mambaforge.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<YOUR COMMAND HERE>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<details>\n",
    "  <summary>Click for help</summary>\n",
    "    \n",
    "  Make sure you include the `!` in front of your command!  \n",
    "    \n",
    "\n",
    "`curl -L -O https://github.com/conda-forge/miniforge/releases/latest/download/Mambaforge-$(uname)-$(uname -m).sh` \n",
    "`bash Mambaforge-$(uname)-$(uname -m).sh -b -u -p $HOME/mambaforge` \n",
    "`date +\"%T\"` \n",
    "\n",
    "</details>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, add it to the path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<YOUR COMMAND HERE>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>Click for help</summary>\n",
    "    \n",
    "    \n",
    "\n",
    "`#add to your path`  \n",
    "`import os`  \n",
    "`os.environ[\"PATH\"] += os.pathsep + os.environ[\"HOME\"]+\"/mambaforge/bin\"`  \n",
    "\n",
    "\n",
    "</details>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, using mambaforge and bioconda, install the tools that will be used in this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<YOUR COMMAND HERE>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<details>\n",
    "  <summary>Click for help</summary>\n",
    "    \n",
    "\n",
    "`mamba install -y -c conda-forge -c bioconda trimmomatic fastqc multiqc salmon entrez-direct gffread parallel-fastq-dump sra-tools pigz`\n",
    "\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 2: Setup Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The directory structure will be the same as in Tutorial 1B. Create that below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<YOUR COMMAND HERE>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>Click for help</summary>\n",
    "    \n",
    "\n",
    "` cd $HOMEDIR`  \n",
    "` mkdir -p data`  \n",
    "` mkdir -p data/raw_fastq`  \n",
    "` mkdir -p data/trimmed`  \n",
    "` mkdir -p data/fastqc`  \n",
    "` mkdir -p data/aligned`  \n",
    "` mkdir -p data/reference`  \n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set number of cores depending on your VM size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<YOUR COMMAND HERE>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>Click for help</summary>\n",
    "  \n",
    "\n",
    "`numthreads=!nproc`  \n",
    "`numthreadsint = int(numthreads[0])`  \n",
    "`%env CORES = $numthreadsint`  \n",
    "`echo ${CORES}`  \n",
    "    \n",
    "</details>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### STEP 3: Downloading relevant FASTQ files using SRA Tools\n",
    "\n",
    "As before, we need to download FASTQ data from SRA. We mentioned at the beginning of this tutorial that we are only using 3 out of the 25 time points from the experiment. The Sequence Run Project for this manuscript is [SRP261441](https://www.ncbi.nlm.nih.gov/sra/?term=SRP261441). We're going to select time points 1, 13, and 25. Look through the SRP page and find the accession numbers for those time points. We will use them in the next step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>Click for help</summary>\n",
    "    \n",
    "    Accession numbers are SRR11784387, SRR11784398, and SRR11784410\n",
    "  \n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add those accession numbers to your accs.txt file and have a look at it to make sure they are correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat accs.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Downloading multiple files using the SRA-toolkit.\n",
    "\n",
    "Next we are going to download the FASTQ files using the sra-toolkit. We are going to do this in two steps: prefetch followed by fasterq-dump. Write the command below that will prefetch the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<YOUR COMMAND HERE>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>Click for help</summary>\n",
    "   \n",
    "    \n",
    "   `prefetch -O data/raw_fastq/ --option-file accs.txt `\n",
    "   \n",
    "  \n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting Multiple SRA files to Fastq\n",
    "\n",
    "As before, use a loop to convert each file in the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<YOUR COMMAND HERE>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<details>\n",
    "  <summary>Click for help</summary>\n",
    "   \n",
    "\n",
    "`for x in `cat accs.txt`; do fasterq-dump -f -O data/raw_fastq -e $CORES -m 4G data/raw_fastq/$x/$x.sra; done`  \n",
    "\n",
    "   \n",
    "  \n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert to fastq.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<YOUR COMMAND HERE>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<details>\n",
    "  <summary>Click for help</summary>\n",
    "   \n",
    "\n",
    "`pigz data/raw_fastq/*.fastq`  \n",
    "\n",
    "   \n",
    "  \n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### STEP 4: Copy reference transcriptome files that will be used by Salmon using E-Direct\n",
    "\n",
    "As before, we are going to use Salmon to quantify the transcriptome. We will need a Refseq assembly here. Since these reads are on the _Plasmodium falciparum_ organism, we need to update our Refseq files. Try searching through [NCBI genome database](https://www.ncbi.nlm.nih.gov/datasets/genome) for the appropriate _Plasmodium falciparum_ assembly.\n",
    "\n",
    "Once you find an assembly, download transcriptome reference files from that assembly using FTP links.\n",
    "\n",
    "Try repurposing the esearch commands from Tutorial1B to download the files for _P. falciparum_. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<YOUR COMMAND HERE>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>Click for help</summary>\n",
    "   \n",
    "\n",
    "`#parse for the ftp link and download the genome reference fasta file`  \n",
    "`esearch -db assembly -query GCF_000002765.6 | efetch -format docsum \\`  \n",
    "`| xtract -pattern DocumentSummary -element FtpPath_RefSeq \\`  \n",
    "`| awk -F\"/\" '{print \"curl -o data/reference/\"$NF\"_genomic.fna.gz \" $0\"/\"$NF\"_genomic.fna.gz\"}' \\`  \n",
    "`| bash`  \n",
    "\n",
    "`#parse for the ftp link and download the gtf reference fasta file`  \n",
    "`esearch -db assembly -query GCF_000002765.6 | efetch -format docsum \\`  \n",
    "`| xtract -pattern DocumentSummary -element FtpPath_RefSeq \\`  \n",
    "`| awk -F\"/\" '{print \"curl -o data/reference/\"$NF\"_genomic.gff.gz \" $0\"/\"$NF\"_genomic.gff.gz\"}' \\`  \n",
    "`| bash`  \n",
    "\n",
    "`# parse for the ftp link and download the feature-table reference file`  \n",
    "`# (for later use for merging readcounts with gene names in R code).`\n",
    "`esearch -db assembly -query GCF_000002765.6 | efetch -format docsum \\`  \n",
    "`| xtract -pattern DocumentSummary -element FtpPath_RefSeq \\`  \n",
    "`| awk -F\"/\" '{print \"curl -o data/reference/\"$NF\"_feature_table.txt.gz \" $0\"/\"$NF\"_feature_table.txt.gz\"}' \\`  \n",
    "`| bash`  \n",
    "\n",
    "\n",
    "`#unzip the compresseed fasta files`  \n",
    "`gzip -d data/reference/*.gz --force`  \n",
    "   \n",
    "  \n",
    "</details>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now create a transcriptome reference file from the files we downloaded using the `gffread` command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<YOUR COMMAND HERE>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>Click for help</summary>\n",
    "   \n",
    "\n",
    "`gffread -w data/reference/GCF_000002765.6_transcriptome_reference.fa -g data/reference/GCF_000002765.6_GCA_000002765_genomic.fna`   `data/reference/GCF_000002765.6_GCA_000002765_genomic.gff`  \n",
    "\n",
    "   \n",
    "  \n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is also recommended to include the full genome at the end of the transcriptome reference file, for the purpose of performing a 'decoy-aware' mapping, more information about which can be found in the Salmon documentation. Note that a key difference between this and the first tutorial is that since we are using data from a eukaryotic organism, the genome comes in multiple chromosomes. That means that we will actually be pasting 14 sequences (_P. falciparum_ has 14 chromosomes) to the transcriptome file and writing 14 lines to the decoy file.\n",
    "\n",
    "There are a couple ways you could do this. One easy whay would be to simply copy and past the chromosome names into the decoys.txt file, but this does scale well if you end up using an organism with many chromosomes or if you make a user error. A better way is to programmatically extract the chromosome names from the genome fasta file. Try combining some `grep` and `sed` commands to pull the ID lines from the fasta file and extract the chromosome name from it. If you get stuck, we have one option in the hint box below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>Click for help</summary>\n",
    "   \n",
    "\n",
    "`# First use cat to combine the transcriptome and genome fasta files into a transcriptome_reference_w_decoy fasta file`  \n",
    "`cat data/reference/GCF_000002765.6_transcriptome_reference.fa <(echo) data/reference/GCF_000002765.6_GCA_000002765_genomic.fna > data/reference/GCF_000002765.6_transcriptome_reference_w_decoy.fa`  \n",
    "\n",
    "`# Next, pull out the fasta headers by searching for > and extract the chromosome name from the line of text.`  \n",
    "`cat data/reference/GCF_000002765.6_GCA_000002765_genomic.fna | grep \">\" | sed 's/Plasmodium.*//g' | sed 's/>//g' > data/reference/decoys.txt`  \n",
    "   \n",
    "  \n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 5: Copy data file for Trimmomatic\n",
    "\n",
    "Using the `gsutil` command, copy the TruSeq3-PE.fa file to your working directory. For convenience, a copy of this file is kept at `gs://nigms-sandbox/me-inbre-rnaseq-pipelinev2/config/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<YOUR COMMAND HERE>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 6: Run Trimmomatic\n",
    "Next write a command to run Trimmomatic on each of the sequences. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<YOUR COMMAND HERE>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>Click for help</summary>\n",
    "   \n",
    "\n",
    "You can create a sort of loop by using the `cat` command to read the accession numbers from the `accs.txt` file and piping it into Trimmomatic. Use the `xargs` command to get the output from `cat` into Trimmomatic.\n",
    "   \n",
    "  \n",
    "</details>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 7: Run FastQC\n",
    "Next we run FastQC. You can use the same `cat` command as above to run each sample through FastQC successively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "<YOUR COMMAND HERE>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 8: Run MultiQC\n",
    "Create a MultiQC report from your output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<YOUR COMMAND HERE>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 9: Index the Transcriptome so that Trimmed Reads Can Be Mapped Using Salmon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Over the next few steps we need to index the transcriptome, map the reads to the transcriptom, and quantify the mapping. Using the `salmon index` command, create a Salmon index using the files we assembled above. Try using `salmon index -h` if you need help."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<YOUR COMMAND HERE>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>Click for help</summary>\n",
    "   \n",
    "\n",
    "`salmon index -t data/reference/GCF_000002765.6_transcriptome_reference_w_decoy.fa -p $CORES -i data/reference/transcriptome_index --decoys data/reference/decoys.txt -k 31 --keepDuplicates`  \n",
    "   \n",
    "  \n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 10: Run Salmon to Map Reads to Transcripts and Quantify Expression Levels\n",
    "Run Salmon on each sample using `salmon quant`. You can either create a `for` loop, or use `cat accs.txt` to pipe each sample into Salmon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "<YOUR COMMAND HERE>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>Click for help</summary>\n",
    "   \n",
    "\n",
    "`cat accs.txt | xargs -I {} salmon quant -i data/reference/transcriptome_index -l SR -r \"data/trimmed/{}_1_trimmed.fastq.gz\" -p $CORES --validateMappings -o \"data/quants/{}_quant\"`  \n",
    "\n",
    "   \n",
    "  \n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 11: Report the top 10 most highly expressed genes in the samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Tutorial 1B, several examples are shown of using `head`, `sort`, and `grep` to explore the genes that are highly up- or down-regulated in the samples. Try doing some of that exploring here. Additionally, you can have a look at the paper that produced the data to see what kind of genes they explored and see if you can replicate their results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<YOUR COMMAND HERE>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### STEP 12: Combine Genecounts to a Single Genecount File\n",
    "As before, create a readcounts table that combines the counts for each gene across each sample into one easy to analyze quants file. Salmon provides the `salmon quantmerge` command to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<YOUR COMMAND HERE>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>Click for help</summary>\n",
    "   \n",
    "\n",
    "`# First merge salmon files by number of reads.`  \n",
    "`salmon quantmerge --column numreads --quants data/quants/*_quant -o data/quants/merged_quants.txt`  \n",
    "\n",
    "`# Rename the columns based on yoru samples`  \n",
    "\n",
    "\n",
    "`#Print the new dataframe`  \n",
    "`print(\"An example of a combined genecount outputfile.\")`  \n",
    "`!head data/quants/merged_quants.txt`  \n",
    "   \n",
    "  \n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## <a name=\"workflow\">Additional Workflows</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have read counts per gene, feel free to explore the other notebooks using this dataset. What changes will you need to make in order to make those notebooks work with the new data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cpu.m114",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m114"
  },
  "kernelspec": {
   "display_name": "Python 3 (Local)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
