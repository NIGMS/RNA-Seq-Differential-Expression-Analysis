{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extended RNA-Seq Analysis Training Demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For simplicity and time, The short tutorial workflow uses truncated and partial run data from the Cushman et al., project.\n",
    "\n",
    "The tutorial repeats the short tutorial, but with the full fastq files and includes some extra steps, such as how to download and prepare the transcriptome files used by salmon, alternate ways to navigate the NCBI databases for annotation or reference files you might need, and how to combine salmon outputs at the end into a single genecount file.\n",
    "\n",
    "Full fastq files can be rather large, and so the downloading, extracting, and analysis of them means this tutorial can take over 1 hour 45 minutes to run the code fully. This is part of the reason we have a short and easy introductory tutorial, and this longer more full tutorial for those interested.\n",
    "\n",
    "If this is too lengthy feel free to move on to the snakemake tutorial or the DEG analysis tutorial -- all the files used in the DEG tutorial were created using this extended tutorial workflow.\n",
    "\n",
    "![RNA-Seq workflow](images/rnaseq-workflow.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Objectives\n",
    "\n",
    "* **Install necessary bioinformatics tools:**  Learn to install and manage bioinformatics software using conda.\n",
    "\n",
    "* **Set up a project directory structure:** Organize files efficiently for RNA-Seq analysis.\n",
    "\n",
    "* **Download RNA-Seq data from the SRA:** Utilize `prefetch` and `fasterq-dump` to download and convert SRA data to FASTQ format.  Learn to obtain accession numbers from NCBI databases (both manually and using BigQuery).\n",
    "\n",
    "* **Quality control of raw reads:** Use `FastQC` to assess the quality of raw sequencing reads and `MultiQC` to generate a summary report.\n",
    "\n",
    "* **Read trimming and adapter removal:** Employ `Trimmomatic` to improve read quality by removing adapter sequences and low-quality bases.\n",
    "\n",
    "* **Transcriptome preparation:** Download and prepare a reference transcriptome using `entrez-direct` and `gffread`, including creating a decoy file for Salmon.\n",
    "\n",
    "* **RNA-Seq read alignment and quantification:** Use `Salmon` to align reads to the transcriptome and quantify gene expression levels.\n",
    "\n",
    "* **Gene expression analysis:**  Interpret Salmon output to identify highly expressed genes and analyze the expression of specific genes of interest.\n",
    "\n",
    "* **Combine gene counts:** Merge individual sample quantification results into a single gene count table for downstream analysis (e.g., differential expression)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "**APIs:**\n",
    "\n",
    "* **`aws s3`:** Used extensively to download data from an Amazon S3 bucket.\n",
    "\n",
    "**Software and Dependencies:**\n",
    "\n",
    "* **Conda:** The package manager used for installing most of the bioinformatics tools.  The notebook uses `conda install` commands.\n",
    "* **Trimmomatic:**  Used for quality trimming of raw sequencing reads.\n",
    "* **FastQC:**  A quality control tool for high-throughput sequence data.\n",
    "* **MultiQC:**  Aggregates results from multiple FastQC runs into a single report.\n",
    "* **Salmon:** A tool for quantifying transcript abundances from RNA-Seq data.\n",
    "* **Entrez Direct (EDirect):** NCBI's command-line tool for accessing the Entrez databases (used here to retrieve reference genome information).\n",
    "* **gffread:**  Parses GFF/GTF annotation files and extracts information from them, such as to create a transcriptome reference file from genome and annotation files.\n",
    "* **parallel-fastq-dump:**  A parallel version of the `fastq-dump` tool (part of SRA Toolkit).  It is likely optimized to process multiple files efficiently.\n",
    "* **sra-tools:** NCBI's SRA Toolkit for downloading and processing data from the Sequence Read Archive.  Specifically, `prefetch` and `fasterq-dump` are used here.\n",
    "* **pigz:** A parallel version of the gzip compression/decompression tool.  It's faster for larger files.\n",
    "* **(Optional) Amazon Athena and PyAthena:** If you want to retrieve SRA accession numbers using Amazon Athena instead of using a pre-existing list, you will need this SQL database query service and its python connector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Started"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 1: Install Miniforge and then install snakemake using bioconda."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First install Miniforge.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -L -O https://github.com/conda-forge/miniforge/releases/latest/download/Miniforge3-$(uname)-$(uname -m).sh\n",
    "!bash Miniforge3-$(uname)-$(uname -m).sh -p $HOME/SageMaker/miniforge -b\n",
    "!date +\"%T\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add to your path\n",
    "import os\n",
    "os.environ[\"PATH\"] +=os.pathsep + os.environ[\"HOME\"]+\"/SageMaker/miniforge/bin\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, using miniforge and bioconda, install the tools that will be used in this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! conda install -y -c conda-forge -c bioconda trimmomatic fastqc multiqc salmon entrez-direct gffread parallel-fastq-dump sra-tools=3.0.5 pigz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!prefetch --version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 2: Setup Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a set of directories to store the reads, reference sequence files, and output files. Notice that first we remove the `data` directory to clean up files from Tutorial_1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! cd $HOMEDIR\n",
    "! echo $PWD\n",
    "! rm -r data/\n",
    "! mkdir -p data\n",
    "! mkdir -p data/raw_fastq\n",
    "! mkdir -p data/trimmed\n",
    "! mkdir -p data/fastqc\n",
    "! mkdir -p data/aligned\n",
    "! mkdir -p data/reference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set number of cores depending on your VM size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numthreads=!nproc\n",
    "numthreadsint = int(numthreads[0])\n",
    "%env CORES = $numthreadsint\n",
    "#!echo ${CORES}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 3: Downloading relevant FASTQ files using SRA Tools\n",
    "\n",
    "Next we will need to download the relevant fastq files.\n",
    "\n",
    "Because these files can be large, the process of downloading and extracting fastq files can be quite lengthy.\n",
    "\n",
    "The sequence data for this tutorial comes from work by Cushman et al., <em><a href='https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8191103/'>Increased whiB7 expression and antibiotic resistance in Mycobacterium chelonae carrying two prophages</a><em>.\n",
    "\n",
    "We will be downloading the sample runs from this project using SRA tools, downloading from the NCBI's SRA (Sequence Run Archives).\n",
    "\n",
    "However, first we need to find the associated accession numbers in order to download.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 3.1: Finding run accession numbers.\n",
    "\n",
    "The SRA stores sequence data in terms of runs, (SRR stands for Sequence Read Run). To download runs, we will need the accession ID for each run we wish to download. \n",
    "\n",
    "The Cushman et al., project contains 12 runs. To make it easier, these are the run IDs associated with this project:\n",
    "\n",
    "+ SRR13349122\n",
    "+ SRR13349123\n",
    "+ SRR13349124\n",
    "+ SRR13349125\n",
    "+ SRR13349126\n",
    "+ SRR13349127\n",
    "+ SRR13349128\n",
    "+ SRR13349129\n",
    "+ SRR13349130\n",
    "+ SRR13349131\n",
    "+ SRR13349132\n",
    "+ SRR13349133\n",
    "\n",
    "\n",
    "In this case, all these runs belong to the SRP (Sequence Run Project): SRP300216.\n",
    "\n",
    "Sequence run experiments can be searched for using the SRA database on the NCBI website; and article-specific sample run information can be found in the supplementary section of that article.\n",
    "\n",
    "For instance, here, the the authors posted a link to the sequence data GSE (Gene Series number), <a href='https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE164210'>GSE164210</a>. This leads to the appropriate 'Gene Expression Omnibus' page where, among other useful files and information, the relevant SRA database link can be found. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can download this text file with the accession numbers and continue to STEP 3.2, or you can optionally use BigQuery to generate an accession list following the instructions outlined in [this notebook](https://github.com/STRIDES/NIHCloudLabGCP/blob/main/notebooks/SRADownload/SRA-Download.ipynb).\n",
    "### STEP 3.1.1: Download the accession list file with gsutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! aws s3 cp s3://nigms-sandbox/me-inbre-rnaseq-pipelinev2/accs.txt ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 3.1.2 (Optional): Generate the accession list file with Amazon Athena\n",
    "#### Set up your Athena Database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You need to set up your Athena database in the Athena console before you start this notebook. Follow our [guide](https://github.com/STRIDES/NIHCloudLabAWS/blob/main/docs/create_athena_database.md) to walk you through it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Install PyAthena"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install pyathena"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "from pyathena import connect\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Establish connection\n",
    "List your staging bucket and the region of your bucket. Make sure your bucket is in us-east-1 to avoid egress charges when downloading from sra.  \\\n",
    "You can a create a satging bucket here if you need!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a bucket if you need!\n",
    "# ! aws s3 mb s3://bucket-for-athena-query-test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = connect(s3_staging_dir='s3://bucket-for-athena-query-test/',\n",
    "               region_name='us-east-1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will query Athena using the species name and a range of accession numbers associated with this particular study. Feel free to play around with the query to generate different variations of accession numbers!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT acc\n",
    "FROM AwsDataCatalog.sra_metadata.metadata\n",
    "WHERE organism = 'Mycobacteroides chelonae'\n",
    "AND acc LIKE '%SRR133491%'\n",
    "ORDER BY acc\n",
    "\"\"\"\n",
    "df = pd.read_sql(\n",
    "    query, conn\n",
    ")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write into a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('accs.txt', 'w') as f:\n",
    "    accs = df['acc'].to_string(header=False, index=False)\n",
    "    f.write(accs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat accs.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 3.2: Using the SRA-toolkit for a single sample.\n",
    "\n",
    "Now use the Sequence Run accession ID to download the sequence data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!prefetch SRR13349124 -O data/raw_fastq -f yes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the SRA archives sequence files in the SRA format. \n",
    "\n",
    "Typically genome workflows process data in the form of zipped or unzipped .fastq, or .fasta files\n",
    "\n",
    "So before we move on, we need to convert the files from .sra to .fastq using the fastq-dump tool.\n",
    "\n",
    "We will also compresss the fastq files to make them take less space, making them fastq.gz files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!for x in `cat accs.txt`; do fasterq-dump -f -O data/raw_fastq -e $CORES -m 4G data/raw_fastq/$x/$x.sra; done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 3.3 Downloading multiple files using the SRA-toolkit.\n",
    "\n",
    "One may, as in our case, wish to download multiple runs at once.\n",
    "\n",
    "To aid in this, SRA-tools supports batch downloading.\n",
    "\n",
    "We can download multiple SRA files using a single line of code by creating a list of the SRA IDs we wish to download, and inputting that into the prefetch command."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then feed that list into the sra-toolkit prefetch command. Note, it may take some time to download all the fastq files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!prefetch -O data/raw_fastq/ --option-file accs.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 3.3 Converting Multiple SRA files to Fastq\n",
    "\n",
    "We used fasterq-dump before to convert SRA files to fastq. However, fasterq-dump does not have native batch compatibility. As before, we will use a loop to convert each file in our list. In this case, we are going to convert to fastq.gz for downstream processing. This step should take about 30 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!for x in `cat accs.txt`; do fasterq-dump -f -O data/raw_fastq -e $CORES -m 4G data/raw_fastq/$x/$x.sra; done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert to fastq.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!time pigz data/raw_fastq/*.fastq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 4: Copy reference transcriptome files that will be used by Salmon using E-Direct\n",
    "\n",
    "Salmon is a tool that aligns RNA-Seq reads to a transcriptome.\n",
    "\n",
    "So we will need a transcriptome reference file.\n",
    "\n",
    "To get one, we can search through the NCBI assembly database, find an assembly, and download transcriptome reference files from that assembly using FTP links.\n",
    "\n",
    "For instance, we will use the <a href='https://www.ncbi.nlm.nih.gov/assembly/GCF_001632805.1'>ASM163280v1</a> refseq assembly, found by searching through the NCBI assembly database. The FTP links can be accessed through the website in various ways, one way is to click the 'FTP directory for RefSeq assembly' link, found under 'Access the data', section.\n",
    "\n",
    "Alternatively, if one were inclined, one could take the less common route and perform this through the NCBI command line tool suite called 'Entrez Direct' (EDirect).\n",
    "\n",
    "This is an intricate and complicated set of tools, with many ways to do any one thing.\n",
    "\n",
    "Below is an example of using an eDirect search query with a refseq identifier to obtain the relevant FTP directory, and then using that to download desired reference files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#parse for the ftp link and download the genome reference fasta file\n",
    "\n",
    "!esearch -db assembly -query GCF_001632805.1 | efetch -format docsum \\\n",
    "| xtract -pattern DocumentSummary -element FtpPath_RefSeq \\\n",
    "| awk -F\"/\" '{print \"curl -o data/reference/\"$NF\"_genomic.fna.gz \" $0\"/\"$NF\"_genomic.fna.gz\"}' \\\n",
    "| bash\n",
    "\n",
    "#parse for the ftp link and download the gtf reference fasta file\n",
    "\n",
    "!esearch -db assembly -query GCF_001632805.1 | efetch -format docsum \\\n",
    "| xtract -pattern DocumentSummary -element FtpPath_RefSeq \\\n",
    "| awk -F\"/\" '{print \"curl -o data/reference/\"$NF\"_genomic.gff.gz \" $0\"/\"$NF\"_genomic.gff.gz\"}' \\\n",
    "| bash\n",
    "\n",
    "# parse for the ftp link and download the feature-table reference file \n",
    "# (for later use for merging readcounts with gene names in R code).\n",
    "\n",
    "!esearch -db assembly -query GCF_001632805.1 | efetch -format docsum \\\n",
    "| xtract -pattern DocumentSummary -element FtpPath_RefSeq \\\n",
    "| awk -F\"/\" '{print \"curl -o data/reference/\"$NF\"_feature_table.txt.gz \" $0\"/\"$NF\"_feature_table.txt.gz\"}' \\\n",
    "| bash\n",
    "\n",
    "\n",
    "#unzip the compresseed fasta files\n",
    "\n",
    "!gzip -d data/reference/*.gz --force"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we can use a tool called gffread to create a transcriptome reference file using the gtf and genome files we downloaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gffread -w data/reference/GCF_001632805.1_transcriptome_reference.fa -g data/reference/GCF_001632805.1_ASM163280v1_genomic.fna data/reference/GCF_001632805.1_ASM163280v1_genomic.gff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is also recommended to include the full genome at the end of the transcriptome reference file, for the purpose of performing a 'decoy-aware' mapping, more information about which can be found in the Salmon documentation.\n",
    "\n",
    "To alert the tool to the presence of this, we will also create a 'decoy file', which salmon needs pointed towards the full genome sequence in our transcriptome reference file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat data/reference/GCF_001632805.1_transcriptome_reference.fa <(echo) data/reference/GCF_001632805.1_ASM163280v1_genomic.fna > data/reference/GCF_001632805.1_transcriptome_reference_w_decoy.fa\n",
    "!echo \"NZ_CP007220.1\" > data/reference/decoys.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 5: Copy data file for Trimmomatic\n",
    "\n",
    "One of trimmomatics functions is to trim sequence machine specific adapter sequences. These are usually within the trimmomatic installation directory in a folder called adapters.\n",
    "\n",
    "Directories of packages within conda installations can be confusing, so in the case of using conda with trimmomatic, it may be easier to simply download or create a file with the relevant adapter sequencecs and store it in an easy to find directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! gsutil -m cp -r gs://nigms-sandbox/me-inbre-rnaseq-pipelinev2/config/TruSeq3-PE.fa .\n",
    "!head TruSeq3-PE.fa "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 6: Run Trimmomatic\n",
    "Trimmomatic will trim off any adapter sequences or low quality sequence it detects in the FASTQ files.\n",
    "\n",
    "Using piping and our original list, it is possible to queue up a batch run of trimmomatic for all our files, note that this is a different way to run a loop compared with what we did before.\n",
    "\n",
    "The below code may take approximately 35 minutes to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat accs.txt | xargs -I {} trimmomatic PE -threads $CORES 'data/raw_fastq/{}_1.fastq.gz' 'data/raw_fastq/{}_2.fastq.gz' 'data/trimmed/{}_1_trimmed.fastq.gz' 'data/trimmed/{}_1_trimmed_unpaired.fastq.gz' 'data/trimmed/{}_2_trimmed.fastq.gz' 'data/trimmed/{}_2_trimmed_unpaired.fastq.gz' ILLUMINACLIP:TruSeq3-PE.fa:2:30:10:2:keepBothReads LEADING:3 TRAILING:3 MINLEN:36"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 7: Run FastQC\n",
    "FastQC is an invaluable tool that allows you to evaluate whether there are problems with a set of reads. For example, it will provide a report of whether there is any bias in the sequence composition of the reads.\n",
    "\n",
    "If you notice the results of the trimming, you may have noted the sequences in the reverse reads were few, and largely unpaired. This may be an artifact from how the original sequencing process. This is okay, we can proceed from here simply using the forward reads.\n",
    "\n",
    "The below code may take around 10 minutes to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat accs.txt | xargs -I {} fastqc -t $CORES \"data/trimmed/{}_1_trimmed.fastq.gz\" -o data/fastqc/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 8: Run MultiQC\n",
    "MultiQC reads in the FastQC reports and generate a compiled report for all the analyzed FASTQ files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!multiqc -f data/fastqc/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 9: Index the Transcriptome so that Trimmed Reads Can Be Mapped Using Salmon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!salmon index -t data/reference/GCF_001632805.1_transcriptome_reference_w_decoy.fa -p $CORES -i data/reference/transcriptome_index --decoys data/reference/decoys.txt -k 31 --keepDuplicates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 10: Run Salmon to Map Reads to Transcripts and Quantify Expression Levels\n",
    "Salmon aligns the trimmed reads to the reference transcriptome and generates the read counts per transcript. In this analysis, each gene has a single transcript."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat accs.txt | xargs -I {} salmon quant -i data/reference/transcriptome_index -l SR -r \"data/trimmed/{}_1_trimmed.fastq.gz\" -p $CORES --validateMappings -o \"data/quants/{}_quant\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 11: Report the top 10 most highly expressed genes in the samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Top 10 most highly expressed genes in each wild-type sample.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head data/quants/SRR13349122_quant/quant.sf -n 1\n",
    "!sort -nrk 4,4 data/quants/SRR13349122_quant/quant.sf | head -10\n",
    "!sort -nrk 4,4 data/quants/SRR13349123_quant/quant.sf | head -10\n",
    "!sort -nrk 4,4 data/quants/SRR13349124_quant/quant.sf | head -10\n",
    "!sort -nrk 4,4 data/quants/SRR13349125_quant/quant.sf | head -10\n",
    "!sort -nrk 4,4 data/quants/SRR13349126_quant/quant.sf | head -10\n",
    "!sort -nrk 4,4 data/quants/SRR13349127_quant/quant.sf | head -10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Top 10 most highly expressed genes in the double lysogen samples.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head data/quants/SRR13349122_quant/quant.sf -n 1\n",
    "!sort -nrk 4,4 data/quants/SRR13349128_quant/quant.sf | head -10\n",
    "!sort -nrk 4,4 data/quants/SRR13349129_quant/quant.sf | head -10\n",
    "!sort -nrk 4,4 data/quants/SRR13349130_quant/quant.sf | head -10\n",
    "!sort -nrk 4,4 data/quants/SRR13349131_quant/quant.sf | head -10\n",
    "!sort -nrk 4,4 data/quants/SRR13349132_quant/quant.sf | head -10\n",
    "!sort -nrk 4,4 data/quants/SRR13349133_quant/quant.sf | head -10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 12: Report the expression of a putative acyl-ACP desaturase (BB28_RS16545) that was downregulated in the double lysogen relative to wild-type\n",
    "A acyl-transferase was reported to be downregulated in the double lysogen as shown in the table of the top 20 upregulated and downregulated genes from the paper describing the study.\n",
    "![RNA-Seq workflow](images/table-cushman.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use `grep` to report the expression in the wild-type sample. The fields in the Salmon `quant.sf` file are as follows. The level of expression is reported in the Transcripts Per Million (`TPM`) and number of reads (`NumReads`) fields:  \n",
    "`Name    Length  EffectiveLength TPM     NumReads`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!grep 'BB28_RS16545' data/quants/SRR13349122_quant/quant.sf\n",
    "!grep 'BB28_RS16545' data/quants/SRR13349123_quant/quant.sf\n",
    "!grep 'BB28_RS16545' data/quants/SRR13349124_quant/quant.sf\n",
    "!grep 'BB28_RS16545' data/quants/SRR13349125_quant/quant.sf\n",
    "!grep 'BB28_RS16545' data/quants/SRR13349126_quant/quant.sf\n",
    "!grep 'BB28_RS16545' data/quants/SRR13349127_quant/quant.sf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use `grep` to report the expression in the double lysogen sample. The fields in the Salmon `quant.sf` file are as follows. The level of expression is reported in the Transcripts Per Million (`TPM`) and number of reads (`NumReads`) fields:  \n",
    "`Name    Length  EffectiveLength TPM     NumReads`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!grep 'BB28_RS16545' data/quants/SRR13349128_quant/quant.sf\n",
    "!grep 'BB28_RS16545' data/quants/SRR13349129_quant/quant.sf\n",
    "!grep 'BB28_RS16545' data/quants/SRR13349130_quant/quant.sf\n",
    "!grep 'BB28_RS16545' data/quants/SRR13349131_quant/quant.sf\n",
    "!grep 'BB28_RS16545' data/quants/SRR13349132_quant/quant.sf\n",
    "!grep 'BB28_RS16545' data/quants/SRR13349133_quant/quant.sf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 12: Combine Genecounts to a Single Genecount File\n",
    "Commonly, the readcounts for each sample are combined into a single table, where the rows contain the gene ID, and the columns identify the sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##first merge salmon files by number of reads.\n",
    "!salmon quantmerge --column numreads --quants data/quants/*_quant -o data/quants/merged_quants.txt\n",
    "##optinally we can rename the columns\n",
    "!sed -i \"1s/.*/Name\\tSRR13349122\\tSRR13349123\\tSRR13349124\\tSRR13349125\\tSRR13349126\\tSRR13349127\\tSRR13349128\\tSRR13349129\\tSRR13349130\\tSRR13349131\\tSRR13349132\\tSRR13349133/\" data/quants/merged_quants.txt\n",
    "\n",
    "##for further formatting, it may be easier in our r-code to later merge\n",
    "##if we remove the gene- and rna- prefix\n",
    "!sed -i \"s/gene-//\" data/quants/merged_quants.txt\n",
    "!sed -i \"s/rna-//\" data/quants/merged_quants.txt\n",
    "\n",
    "print(\"An example of a combined genecount outputfile.\")\n",
    "!head data/quants/merged_quants.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"workflow\">Additional Workflows</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have read counts per gene, feel free to explore the R workflow which creates plots and analyses using these readcount files, or try other alternate workflows for creating read count files, such as using snakemake."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "[Workflow One:](Tutorial_1.ipynb) A short introduction to downloading and mapping sequences to a transcriptome using Trimmomatic and Salmon. Here is a link to the YouTube video demonstrating the tutorial: <https://youtu.be/ChGfBR4do_Y>.\n",
    "\n",
    "[Workflow One (Extended):](Tutorial_1B_Extended.ipynb) An extended version of workflow one. Once you have got your feet wet, you can retry workflow one with this extended version that covers the entire dataset, and includes elaboration such as using SRA tools for sequence downloading, and examples of running batches of fastq files through the pipeline. This workflow may take around an hour to run.\n",
    "\n",
    "[Workflow One (Using Snakemake):](Tutorial_2_Snakemake.ipynb) Using snakemake to run workflow one.\n",
    "\n",
    "[Workflow Two (DEG Analysis):](Tutorial_3_DEG_Analysis.ipynb) Using Deseq2 and R to conduct clustering and differential gene expression analysis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![RNA-Seq workflow](images/RNA-Seq_Notebook_Homepage.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This extended RNA-Seq analysis tutorial provided a comprehensive workflow for processing RNA-Seq data, from raw SRA files to a combined gene count table suitable for differential gene expression analysis.  We demonstrated the use of various bioinformatics tools, including `prefetch`, `fasterq-dump`, `Trimmomatic`, `FastQC`, `MultiQC`, `Salmon`, `entrez-direct`, and `gffread`,  highlighting best practices for data management and efficient command-line execution, particularly for batch processing of multiple samples. The tutorial included detailed steps for downloading data from the NCBI SRA database, utilizing both manual accession identification and an optional approach leveraging Amazon Athena for programmatic retrieval.  The integration of these tools facilitated quality control, read trimming, transcriptome preparation, read alignment, and quantification, culminating in a consolidated gene count file.  This workflow, although time-consuming due to the processing of full FASTQ datasets, provides a robust foundation for more advanced analyses such as those detailed in the subsequent Snakemake and DEG analysis tutorials.  The resulting gene count matrix serves as a crucial input for downstream differential expression analyses, building upon the knowledge gained in this comprehensive guide."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean Up\n",
    "\n",
    "Remember to move to the next notebook or shut down your instance if you are finished."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 4
}
