{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extended RNA-Seq Analysis Training Demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, we will repeat the Tutorial 1B extended submodule with a different dataset. Using the provided dataset is a great way to get introduced to the module and learn its basic functionality, but to use it with your own data or to understand the module at a deeper level, it is helpful to practice adapting it to a new dataset. We will guide you through that here, providing basic guidance on the workflow and letting you write the commands yourself. If you get stuck, you can always view our hints and suggestions by clicking on the dropdown arrow under the command cells.\n",
    "\n",
    "The initial dataset used a prokaryotic sample for simplicity and compute efficiency. This time, we will increase the complexity a bit by using a eukaryotic sample. We have selected data from a time series experiment on Plasmodium falciparum, the parasite responsible for malaria. The experiment did RNA-seq analysis on P. falciparum cells at 25 time points after erythrocyte invasion. The data is taken from Kucharski M, Tripathi J, Nayak S, Zhu L et al. A comprehensive RNA handling and transcriptomics guide for high-throughput processing of Plasmodium blood-stage samples. Malar J 2020 Oct 9;19(1):363. The sequence data is available from SRA with the accession number [SRP261441](https://www.ncbi.nlm.nih.gov/sra/?term=SRP261441). To keep things simple, we are not using all time points, and have selected time points 1, 13, and 25 as a proxy for early, mid, and late infection. Feel free to add or remove samples from your analysis to see how the results differ. The workflow structure remains the same as the original submodule 1B, with the diagram shown below.\n",
    "\n",
    "![RNA-Seq workflow](images/rnaseq-workflow.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Objectives\n",
    "\n",
    "\n",
    "* **Data Acquisition and Management:** Downloading FASTQ files from the SRA database using `prefetch` and `fasterq-dump`, and organizing data into a well-structured directory.  This emphasizes data organization and efficient retrieval.\n",
    "\n",
    "* **Reference Genome Preparation:** Downloading and preparing a reference transcriptome for *Plasmodium falciparum* from NCBI using Entrez Direct commands. This includes using `gffread` to create a transcriptome FASTA file and creating a decoy file containing the chromosome names for decoy-aware mapping in Salmon.  This highlights the importance of appropriate reference data and its preparation.\n",
    "\n",
    "* **Read Processing and Quality Control:** Performing quality control on raw reads using Trimmomatic for adapter trimming and FastQC for quality assessment, and summarizing results with MultiQC.  This emphasizes the necessity of data quality checks.\n",
    "\n",
    "* **Transcript Quantification:**  Creating a Salmon index and using Salmon to quantify transcript abundance from the trimmed reads. This involves understanding the parameters and options for `salmon index` and `salmon quant`. This focuses on the core RNA-Seq quantification step.\n",
    "\n",
    "* **Data Exploration and Interpretation:** Analyzing the resulting gene counts, identifying highly expressed genes using basic command-line tools (`head`, `sort`, `grep`), and comparing results with findings from the original research paper.  This focuses on basic data interpretation and exploration.\n",
    "\n",
    "* **Data Aggregation:** Combining gene counts from multiple samples into a single file using `salmon quantmerge`. This emphasizes efficient data handling and preparation for downstream analysis.\n",
    "\n",
    "* **Adapting a Workflow:** The overarching objective is to adapt a previously learned RNA-Seq workflow (implied from the reference to \"Tutorial 1B\") to a new, more complex dataset.  This is the most significant learning objectiveâ€”transferring knowledge and problem-solving skills."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "**APIs:**\n",
    "\n",
    "* **NCBI SRA API:**  The notebook downloads FASTQ files from the Sequence Read Archive (SRA) using the `prefetch` and `fasterq-dump` commands from the SRA Toolkit.  This implicitly uses the NCBI SRA API.  No explicit API key is shown, suggesting the notebook relies on the default, rate-limited public access.  For large-scale downloads, a faster, higher-throughput method might be needed (consider using a dedicated SRA download client).\n",
    "* **NCBI E-utilities API:** The notebook uses `esearch` and `efetch` commands to retrieve information and download reference genome files from the NCBI databases.  This leverages the NCBI E-utilities API. Again, no explicit key is used, implying reliance on the public API.\n",
    "\n",
    "\n",
    "**Cloud Platform Access:**\n",
    "\n",
    "* **Cloud Storage:** Access to a cloud storage bucket to store the input data (FASTQ files), intermediate results (trimmed reads, quality control reports), reference genome, and the final analysis outputs.  The `aws s3` command indicates Amazon S3 Storage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 1: Install the tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using mamba and bioconda, install the tools that will be used in this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<YOUR COMMAND HERE>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>Click for help</summary>\n",
    "    \n",
    "\n",
    "`mamba install -y -c conda-forge -c bioconda trimmomatic fastqc multiqc salmon entrez-direct gffread parallel-fastq-dump sra-tools pigz`\n",
    "\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 2: Setup Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The directory structure will be the same as in Tutorial 1B. Create that below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<YOUR COMMAND HERE>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>Click for help</summary>\n",
    "    \n",
    "\n",
    "` mkdir -p data`  \n",
    "` mkdir -p data/raw_fastq`  \n",
    "` mkdir -p data/trimmed`  \n",
    "` mkdir -p data/fastqc`  \n",
    "` mkdir -p data/aligned`  \n",
    "` mkdir -p data/reference`  \n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set number of cores depending on your VM size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<YOUR COMMAND HERE>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>Click for help</summary>\n",
    "  \n",
    "\n",
    "`numthreads=!nproc`  \n",
    "`numthreadsint = int(numthreads[0])`  \n",
    "`%env CORES = $numthreadsint`  \n",
    "`echo ${CORES}`  \n",
    "    \n",
    "</details>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 3: Downloading relevant FASTQ files using SRA Tools\n",
    "\n",
    "As before, we need to download FASTQ data from SRA. We mentioned at the beginning of this tutorial that we are only using 3 out of the 25 time points from the experiment. The Sequence Run Project for this manuscript is [SRP261441](https://www.ncbi.nlm.nih.gov/sra/?term=SRP261441). We're going to select time points 1, 13, and 25. Look through the SRP page and find the accession numbers for those time points. We will use them in the next step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>Click for help</summary>\n",
    "    \n",
    "    Accession numbers are SRR11784387, SRR11784398, and SRR11784410\n",
    "  \n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add those accession numbers to your accs.txt file and have a look at it to make sure they are correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat accs.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloading multiple files using the SRA-toolkit.\n",
    "\n",
    "Next we are going to download the FASTQ files using the sra-toolkit. We are going to do this in two steps: prefetch followed by fasterq-dump. Write the command below that will prefetch the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<YOUR COMMAND HERE>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>Click for help</summary>\n",
    "   \n",
    "    \n",
    "   `prefetch -O data/raw_fastq/ --option-file accs.txt `\n",
    "   \n",
    "  \n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting Multiple SRA files to Fastq\n",
    "\n",
    "As before, use a loop to convert each file in the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<YOUR COMMAND HERE>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Click for help</summary>\n",
    "<pre>\n",
    "<code>for x in `cat accs.txt`; do fasterq-dump -f -O data/raw_fastq -e $CORES -m 4G data/raw_fastq/$x/$x.sra; done </code>\n",
    "</pre>\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert to fastq.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<YOUR COMMAND HERE>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>Click for help</summary>\n",
    "   \n",
    "\n",
    "`pigz data/raw_fastq/*.fastq`  \n",
    "\n",
    "   \n",
    "  \n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 4: Copy reference transcriptome files that will be used by Salmon using E-Direct\n",
    "\n",
    "As before, we are going to use Salmon to quantify the transcriptome. We will need a Refseq assembly here. Since these reads are on the _Plasmodium falciparum_ organism, we need to update our Refseq files. Try searching through [NCBI genome database](https://www.ncbi.nlm.nih.gov/datasets/genome) for the appropriate _Plasmodium falciparum_ assembly.\n",
    "\n",
    "Once you find an assembly, download transcriptome reference files from that assembly using FTP links.\n",
    "\n",
    "Try repurposing the esearch commands from Tutorial1B to download the files for _P. falciparum_. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<YOUR COMMAND HERE>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>Click for help</summary>\n",
    "   \n",
    "\n",
    "`#parse for the ftp link and download the genome reference fasta file`   \n",
    "`esearch -db assembly -query GCF_000002765.6 | efetch -format docsum \\`  \n",
    "`| xtract -pattern DocumentSummary -element FtpPath_RefSeq \\`  \n",
    "`| awk -F\"/\" '{print \"curl -o data/reference/\"$NF\"_genomic.fna.gz \" $0\"/\"$NF\"_genomic.fna.gz\"}' \\`  \n",
    "`| bash`  \n",
    "\n",
    "`#parse for the ftp link and download the gtf reference fasta file`   \n",
    "`esearch -db assembly -query GCF_000002765.6 | efetch -format docsum \\`  \n",
    "`| xtract -pattern DocumentSummary -element FtpPath_RefSeq \\`  \n",
    "`| awk -F\"/\" '{print \"curl -o data/reference/\"$NF\"_genomic.gff.gz \" $0\"/\"$NF\"_genomic.gff.gz\"}' \\`  \n",
    "`| bash`  \n",
    "\n",
    "`# parse for the ftp link and download the feature-table reference file`   \n",
    "`# (for later use for merging readcounts with gene names in R code).`\n",
    "`esearch -db assembly -query GCF_000002765.6 | efetch -format docsum \\`  \n",
    "`| xtract -pattern DocumentSummary -element FtpPath_RefSeq \\`  \n",
    "`| awk -F\"/\" '{print \"curl -o data/reference/\"$NF\"_feature_table.txt.gz \" $0\"/\"$NF\"_feature_table.txt.gz\"}' \\`  \n",
    "`| bash`  \n",
    "\n",
    "\n",
    "`#unzip the compresseed fasta files`  \n",
    "`gzip -d data/reference/*.gz --force`  \n",
    "   \n",
    "  \n",
    "</details>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now create a transcriptome reference file from the files we downloaded using the `gffread` command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<YOUR COMMAND HERE>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>Click for help</summary>\n",
    "   \n",
    "\n",
    "`gffread -w data/reference/GCF_000002765.6_transcriptome_reference.fa -g data/reference/GCF_000002765.6_GCA_000002765_genomic.fna`   `data/reference/GCF_000002765.6_GCA_000002765_genomic.gff`  \n",
    "\n",
    "   \n",
    "  \n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is also recommended to include the full genome at the end of the transcriptome reference file, for the purpose of performing a 'decoy-aware' mapping, more information about which can be found in the Salmon documentation. Note that a key difference between this and the first tutorial is that since we are using data from a eukaryotic organism, the genome comes in multiple chromosomes. That means that we will actually be pasting 14 sequences (_P. falciparum_ has 14 chromosomes) to the transcriptome file and writing 14 lines to the decoy file.\n",
    "\n",
    "There are a couple ways you could do this. One easy whay would be to simply copy and past the chromosome names into the decoys.txt file, but this does scale well if you end up using an organism with many chromosomes or if you make a user error. A better way is to programmatically extract the chromosome names from the genome fasta file. Try combining some `grep` and `sed` commands to pull the ID lines from the fasta file and extract the chromosome name from it. If you get stuck, we have one option in the hint box below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>Click for help</summary>\n",
    "   \n",
    "\n",
    "First use cat to combine the transcriptome and genome fasta files into a transcriptome_reference_w_decoy fasta file   \n",
    "    \n",
    "`cat data/reference/GCF_000002765.6_transcriptome_reference.fa <(echo) data/reference/GCF_000002765.6_GCA_000002765_genomic.fna > data/reference/GCF_000002765.6_transcriptome_reference_w_decoy.fa`  \n",
    "\n",
    "Next, pull out the fasta headers by searching for > and extract the chromosome name from the line of text.    \n",
    "    \n",
    "`cat data/reference/GCF_000002765.6_GCA_000002765_genomic.fna | grep \">\" | sed 's/Plasmodium.*//g' | sed 's/>//g' > data/reference/decoys.txt`  \n",
    "   \n",
    "  \n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 5: Copy data file for Trimmomatic\n",
    "\n",
    "Using the `aws s3` command, copy the TruSeq3-PE.fa file to your working directory. For convenience, a copy of this file is kept at `s3://nigms-sandbox/me-inbre-rnaseq-pipelinev2/config/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<YOUR COMMAND HERE>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 6: Run Trimmomatic\n",
    "Next write a command to run Trimmomatic on each of the sequences. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<YOUR COMMAND HERE>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>Click for help</summary>\n",
    "   \n",
    "\n",
    "You can create a sort of loop by using the `cat` command to read the accession numbers from the `accs.txt` file and piping it into Trimmomatic. Use the `xargs` command to get the output from `cat` into Trimmomatic.\n",
    "   \n",
    "  \n",
    "</details>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 7: Run FastQC\n",
    "Next we run FastQC. You can use the same `cat` command as above to run each sample through FastQC successively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<YOUR COMMAND HERE>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 8: Run MultiQC\n",
    "Create a MultiQC report from your output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<YOUR COMMAND HERE>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 9: Index the Transcriptome so that Trimmed Reads Can Be Mapped Using Salmon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Over the next few steps we need to index the transcriptome, map the reads to the transcriptom, and quantify the mapping. Using the `salmon index` command, create a Salmon index using the files we assembled above. Try using `salmon index -h` if you need help."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<YOUR COMMAND HERE>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>Click for help</summary>\n",
    "   \n",
    "\n",
    "`salmon index -t data/reference/GCF_000002765.6_transcriptome_reference_w_decoy.fa -p $CORES -i data/reference/transcriptome_index --decoys data/reference/decoys.txt -k 31 --keepDuplicates`  \n",
    "   \n",
    "  \n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 10: Run Salmon to Map Reads to Transcripts and Quantify Expression Levels\n",
    "Run Salmon on each sample using `salmon quant`. You can either create a `for` loop, or use `cat accs.txt` to pipe each sample into Salmon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<YOUR COMMAND HERE>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>Click for help</summary>\n",
    "   \n",
    "\n",
    "`cat accs.txt | xargs -I {} salmon quant -i data/reference/transcriptome_index -l SR -r \"data/trimmed/{}_1_trimmed.fastq.gz\" -p $CORES --validateMappings -o \"data/quants/{}_quant\"`  \n",
    "\n",
    "   \n",
    "  \n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 11: Report the top 10 most highly expressed genes in the samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Tutorial 1B, several examples are shown of using `head`, `sort`, and `grep` to explore the genes that are highly up- or down-regulated in the samples. Try doing some of that exploring here. Additionally, you can have a look at the paper that produced the data to see what kind of genes they explored and see if you can replicate their results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<YOUR COMMAND HERE>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 12: Combine Genecounts to a Single Genecount File\n",
    "As before, create a readcounts table that combines the counts for each gene across each sample into one easy to analyze quants file. Salmon provides the `salmon quantmerge` command to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<YOUR COMMAND HERE>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>Click for help</summary>\n",
    "   \n",
    "\n",
    "`# First merge salmon files by number of reads.`  \n",
    "`salmon quantmerge --column numreads --quants data/quants/*_quant -o data/quants/merged_quants.txt`  \n",
    "\n",
    "`# Rename the columns based on yoru samples`  \n",
    "\n",
    "\n",
    "`#Print the new dataframe`  \n",
    "`print(\"An example of a combined genecount outputfile.\")`  \n",
    "`!head data/quants/merged_quants.txt`  \n",
    "   \n",
    "  \n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"workflow\">Additional Workflows</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have read counts per gene, feel free to explore the other notebooks using this dataset. What changes will you need to make in order to make those notebooks work with the new data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This Jupyter Notebook provided a hands-on tutorial extending the RNA-Seq analysis workflow from Tutorial 1B to a more complex eukaryotic dataset, specifically a *Plasmodium falciparum* time series experiment.  The tutorial successfully guided users through adapting the pipeline, emphasizing practical application and deeper understanding of the module.  By utilizing data from the SRA (SRP261441), users gained experience in downloading, processing, and quantifying RNA-Seq data from a eukaryotic organism. The use of tools like Trimmomatic, FastQC, MultiQC, and Salmon, along with commands like `entrez-direct`, `gffread`, and `salmon quantmerge`, were demonstrated to efficiently handle the data. The final step involved generating a combined gene count file, ready for further downstream analysis. This tutorial served as a valuable stepping stone, enabling users to apply the learned RNA-Seq analysis skills to their own research projects and fostering a deeper comprehension of the underlying bioinformatics principles.  The notebook encourages further exploration using provided additional workflows and adapting the methods to other datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean Up\n",
    "\n",
    "Remember to shut down your instance if you are finished."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 4
}
