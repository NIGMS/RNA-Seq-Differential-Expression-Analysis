{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "99cfa998-06b6-4b42-ae3a-b4e011750d31",
   "metadata": {},
   "source": [
    "# RNA-Seq Analysis using Snakemake and Google Cloud Life Sciences API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4126cb07-34ee-4780-838f-872015a882b3",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f15ea992-faa6-4705-8384-eb5d81f5daff",
   "metadata": {},
   "source": [
    "This short tutorial demonstrates how to run an RNA-Seq workflow using a prokaryotic data set. Steps in the workflow include read trimming, read QC, read mapping, and counting mapped reads per gene to quantitate gene expression. This tutorial uses a popular workflow manager called [Nextflow](https://www.nextflow.io) run via [Google Batch](https://cloud.google.com/batch/docs/get-started). If you completed the other tutorials in this repo, you will see that it is similar to Tutorial 2, but instead of running Snakemake locally, we switch to Nextflow and run it using Batch in a serverless manner. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c5d3e16",
   "metadata": {},
   "source": [
    "## Learning Objectives\n",
    "\n",
    "* **Installing and configuring Nextflow:** Learners will install mambaforge (a conda distribution) and Nextflow, a workflow management system, within their Jupyter environment. They will also learn how to configure their `nextflow.config` file to utilize the Google Batch execution environment.\n",
    "\n",
    "* **Understanding and modifying a Nextflow pipeline:** The notebook utilizes a pre-existing Nextflow pipeline (`main.nf`).  The objective is to understand how to modify the pipeline's configuration file to point to the correct input data (samplesheet) and output locations within the GCS bucket. This includes setting appropriate parameters for the Google Batch execution (region, project ID, machine type).\n",
    "\n",
    "* **Running a Nextflow workflow on Google Batch:**  Learners will submit their configured Nextflow workflow to Google Batch for execution. This involves understanding and utilizing command-line arguments to specify input files, output directory, working directory, and the execution profile.  They'll also learn about the `-resume` option for restarting partially completed workflows.\n",
    "\n",
    "* **Analyzing RNA-Seq results:**  The final objective is to analyze the generated RNA-Seq data.  This involves using command-line tools (like `grep` and `sort`) to identify and report the top highly expressed genes and the expression levels of specific genes of interest (e.g., a putative acyl-ACP desaturase)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdd443de",
   "metadata": {},
   "source": [
    "This Jupyter notebook performs RNA-Seq analysis using Snakemake (although the notebook itself uses Nextflow) on Google Cloud.  Here's a breakdown of the prerequisites:\n",
    "\n",
    "**APIs that should be enabled:**\n",
    "\n",
    "* **Google Cloud Batch API:**  This is explicitly mentioned in the notebook and is crucial for submitting and managing the Nextflow workflow on Google Batch.\n",
    "* **Google Cloud Compute Engine API:**  While not explicitly stated, it's highly likely needed by Google Batch to provision the virtual machines required for the workflow execution.\n",
    "* **Google Cloud Storage API:**  This is necessary to interact with Google Cloud Storage (GCS) buckets for storing input data, intermediate files, and output results.  The notebook uses `gsutil` commands extensively.\n",
    "* **Google Cloud Logging API:** (mentioned in the notebook)  Likely used for logging information about the batch job execution.\n",
    "\n",
    "**Cloud Platform Account Roles that must be assigned:**\n",
    "\n",
    "The specific roles depend on how the user wants to manage resources. However, at minimum, the user needs roles that grant permissions to:\n",
    "\n",
    "* **Storage Object Admin:**  To create, read, write, and delete objects in the GCS bucket.  This role is needed for uploading inputs and storing outputs.  This could be simplified to a more granular role depending on the needs.\n",
    "* **Batch Job User:**   To submit Batch Jobs.  Without this, the user wouldn't be able to execute the workflow on Google Batch. A more granular role might suffice.\n",
    "* **Compute Instance User:**  To allow Batch to provision VMs. This role may be implicitly granted if the Batch user role is sufficiently powerful. A more granular role might suffice.\n",
    "\n",
    "\n",
    "**Software Prerequisites (not Cloud-specific):**\n",
    "\n",
    "* **Jupyter Notebook:**  The environment to run the notebook itself.\n",
    "* **Nextflow:** The workflow management system used.  The notebook includes instructions for installing it using mambaforge.\n",
    "* **Mambaforge (or Miniconda/Anaconda):**  A package manager for installing Nextflow and its dependencies.\n",
    "* **Docker:**  Docker containers are utilized within the Google Batch jobs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb55ffcf",
   "metadata": {},
   "source": [
    "## Get Started"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b6d0785-2d13-476c-b16a-196f74ea277d",
   "metadata": {},
   "source": [
    "### Step 1: Create a new GS Bucket to store input and output files and Enable the Batch APi\n",
    "Note that your bucket has to be globally unique, so make sure you don't just copy the example here or it won't work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d3dc88f-fa0c-4e7e-972b-055321d3cdbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change this bucket name\n",
    "%env BUCKET=nf-testing-bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17ce680c-4b8c-419c-a6c4-b6caec32d9ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Will only create the bucket if it doesn't yet exist\n",
    "! gsutil ls gs://$BUCKET >& /dev/null || gsutil mb gs://$BUCKET"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6424c54-108f-459e-8f14-f2866bfc0141",
   "metadata": {},
   "source": [
    "Enable the Batch API, likewise you can do this on the console [like this](https://cloud.google.com/batch/docs/get-started#console)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6cb4617-b98c-41b6-995d-711d2f722cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "! gcloud services enable batch.googleapis.com compute.googleapis.com logging.googleapis.com"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd7ab630-955d-43d1-bc43-c7b3e701ed04",
   "metadata": {},
   "source": [
    "### STEP 2: Install nextflow\n",
    "Use mamba to install nextflow. Skip this as needed if you have already completed this step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd5c221b-45ce-47fb-a8e2-29ceee0e296a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Nextflow\n",
    "! mamba install -y -c conda-forge -c bioconda nextflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb0ce8d5-4b96-4e97-88ed-44e8e85f4fc0",
   "metadata": {},
   "source": [
    "### STEP 3: Review input files\n",
    "In order for this tutorial to run quickly, we will only analyze 50,000 reads from a sample from both sample groups instead of analyzing all the reads from all six samples. These files have been posted on a Google Storage Bucket that we made publicly accessible. All other files needed to run the pipeline are also hosted in this public bucket, and will be staged at runtime by Nextflow. To view the locations of all these files, view the `nextflow.config`. You can modify any of these paths as desired, and you could also create a new samplesheet.csv if you want to point the pipeline to different samples. The samplesheet can be stored locally or in a GS bucket."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc33d41c-3444-4e15-944f-07a8625611b7",
   "metadata": {},
   "source": [
    "### STEP 4: Modify config file to allow your project to interact with Google Batch\n",
    "Create and modify your own config file to include a 'gbatch' profile block to tell Nextflow to submit the job to Google Batch instead of running locally.\n",
    "\n",
    "The config file allows nextflow to utilize excecuters like Google Batch. In this tutorial the config files is named 'nextflow.config'. Make sure you open this file and update the <VARIABLES> that are account specific. In this case will will only modify the <PROJECT> with your Project ID. We will specify an outdir and work directory on the command line at run time. \n",
    "\n",
    "Make sure that your region is a region included in the Google Batch!\n",
    "Specify the machine type you would like to use, ensuring that there is enough memory and cpus for the workflow. In this case 16 CPUs is plenty (Otherwise Google Batch will automatically use 1 CPU).\n",
    "```\n",
    "profiles{\n",
    "  gbatch{\n",
    "      process.executor = 'google-batch'\n",
    "      google.location = 'us-central1'\n",
    "      google.region  = 'us-central1'\n",
    "      google.project = '<YOUR_PROJECT>'\n",
    "      process.machineType = 'c2-standard-16'\n",
    "     }\n",
    "}\n",
    "```\n",
    "Note: Make sure your working directory and output directory are different! Google Batch creates temporary file in the working directory within your bucket that do take up space so once your pipeline has completed succesfully feel free to delete the temporary files."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec2c0273-c7f1-4aee-bdf3-43d5773cf2fa",
   "metadata": {},
   "source": [
    "### STEP 5: Submit Nextflow Job to Google Batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc8cb07a-7c7e-430a-9781-600553b3a1e1",
   "metadata": {},
   "source": [
    "A few things to note here: \n",
    "+ --input points to a samplesheet in GS. We could also point to a local samplesheet. This just tells Nextflow where to get the fastq files. \n",
    "+ The profile comes from nextflow.config. It tells the pipeline what to use as execution environment (conda, singularity, or docker) and then you give it a compute environment (in this case gbatch, but if left blank would run locally). \n",
    "+ We specify an outdir. This can point to a local folder if run locally, but since we are using the serverless Google Batch, we need to point the output to a bucket. \n",
    "+ We specify a work dir. Like the outdir, this can be local if run locally, but needs to be in a bucket when running with Batch. \n",
    "+ If you need to rerun your pipeline, you can always add `-resume` and it will search the workdir and not rerun any processes that you have already run. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bee32318-33df-43b2-98bc-5eb091ceae59",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "! nextflow run main.nf --input gs://nigms-sandbox/me-inbre-rnaseq-pipelinev2/data/raw_fastqSub/samplesheet.csv  -profile docker,gbatch  --outdir gs://$BUCKET/outdir/ -w gs://$BUCKET/work/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df9deb0a-1030-4839-aa16-37c3b32a2c87",
   "metadata": {},
   "source": [
    "### STEP 9: Report the top 10 most highly expressed genes in the samples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d50f9bd2-dbd2-467f-a9b6-313e63ad304b",
   "metadata": {},
   "source": [
    "Top 10 most highly expressed genes in the wild-type sample. The level of expression is reported in the Transcripts Per Million (`TPM`) and number of reads (`NumReads`) fields:  \n",
    "`Name    Length  EffectiveLength TPM     NumReads`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b032ce69-f62d-4f5f-90a3-68c2979d9a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "! gsutil ls gs://$BUCKET/outdir/data/quant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c98fd827-6829-400d-af8c-969ad196c3d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "! gsutil cp -r gs://$BUCKET/outdir/data/quant ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fda31107-db3b-4cc2-8183-fc12bfa34e12",
   "metadata": {},
   "source": [
    "View the top 10 most highly expressed genes in the double lysogen sample.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c367b8-5764-4a49-94a5-6f59e3834821",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "for samp in quant/*/quant.sf; \n",
    "    do echo $samp; \n",
    "    sort -nrk 5,5 quant/*/quant.sf | head -10; \n",
    "    done"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50169f62-e707-4d84-b301-ded51a704130",
   "metadata": {},
   "source": [
    "### STEP 10: Report the expression of a putative acyl-ACP desaturase (BB28_RS16545) that was downregulated in the double lysogen relative to wild-type\n",
    "A acyl-transferase was reported to be downregulated in the double lysogen as shown in the table of the top 20 upregulated and downregulated genes from the paper describing the study.\n",
    "![RNA-Seq workflow](images/table-cushman.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b3794b0-a477-45fa-aa51-4414d7671441",
   "metadata": {},
   "source": [
    "Use `grep` to report the expression in the wild-type sample. The fields in the Salmon `quant.sf` file are as follows. The level of expression is reported in the Transcripts Per Million (`TPM`) and number of reads (`NumReads`) fields:  \n",
    "`Name    Length  EffectiveLength TPM     NumReads`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3cb9340-682b-4177-837d-7d803a9775a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "for samp in quant/*/quant.sf; do echo $samp; \n",
    "    echo Name    Length  EffectiveLength TPM     NumReads;\n",
    "    grep 'BB28_RS16545' quant/*/quant.sf; \n",
    "    done"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e119d55",
   "metadata": {},
   "source": [
    "## Conclusion \n",
    "\n",
    "This Jupyter Notebook demonstrated a streamlined RNA-Seq analysis workflow leveraging the power of Nextflow and Google Cloud's Batch API.  The workflow, encompassing read trimming, quality control, mapping, and gene quantification, was executed efficiently in a serverless environment. By utilizing Google Cloud Storage for input and output data, and Google Batch for task orchestration, the notebook showcased a scalable and reproducible approach to RNA-Seq analysis.  The steps detailed the setup, including Google Cloud project configuration and Nextflow installation, followed by the execution of the pipeline and analysis of the resulting gene expression data.  The notebook successfully retrieved and displayed the top 10 most highly expressed genes and demonstrated the analysis of a specific gene of interest, highlighting the practical application of this cloud-based approach for RNA-Seq data processing and interpretation.  This approach offers a robust and scalable solution for researchers needing to efficiently manage computationally intensive RNA-Seq analyses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0063c268",
   "metadata": {},
   "source": [
    "## Clean Up\n",
    "\n",
    "Remember to move to the next notebook or shut down your instance if you are finished."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
